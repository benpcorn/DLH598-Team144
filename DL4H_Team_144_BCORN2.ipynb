{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "Team 144\n",
        "bcorn2@uiuc.edu\n",
        "\n",
        "Github Repo: https://github.com/benpcorn/DLH598-Team144\n",
        "\n",
        "Paper Repo: https://github.com/tufts-ml/SAMIL/tree/main\n",
        "\n",
        "## Problem\n",
        "\n",
        "Huang, Zhe, Wessler, Benjamin S., and Hughes, Michael C. (2023) – Detecting Heart\n",
        "Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance\n",
        "Learning describes the clinical problem of under-diagnosis and under-treatment\n",
        "of aortic stenosis (AS), a degenerative valve condition. In clinical practice, AS is diagnosed\n",
        "by manual expert view of a transthoracic echocardiography (TTE) – which uses ultrasound\n",
        "to produce many images of the heart. AS can be treated effectively, but requires\n",
        "identification early on. If left untreated, \"severe AS has lower 5-year survival rates than several metastatic cancers\" (Huang et al., 2021). When treated, AS has a low mortality rate, but up to 2/3 of symptomatic AS pateints go undiagnosed (Huang et al., 2021). Automatic screening of AS from transthoracic echocardiography\n",
        "imagery can improve the rate of detection and decrease mortality.\n",
        "\n",
        "## Paper Explanation\n",
        "The challenge with automatic detection is each TTE “consists of dozens of images or videos\n",
        "(typically 27-97 in our data) that show\n",
        "the heart’s complex anatomy from different acquisition angles” (Huang, Wessler, and\n",
        "Hughes, 2023) where a clinical expert identifies imagery where the aortic valve is clearly\n",
        "visible, then assesses the severity on a 3-level scale (no, early, significant disease).\n",
        "Traditional Deep Learning approaches classify a single image with a single result, however\n",
        "the clinical expert review makes a single “coherent prediction” (Huang, Wessler, and\n",
        "Hughes, 2023) from knowledge gathered from the set of images. Additionally, the image\n",
        "views produced by a TTE are often unlabeled in Electronic Health Records, further\n",
        "complicating any Deep Learning approaches.\n",
        "\n",
        "The paper finds previous approaches to automatic detection such as attention-based\n",
        "multiple instance learning (MIL) to be insufficient based on accuracy and detection yield,\n",
        "and explores a novel MIL approach to improve the detection of AS from automatic detection\n",
        "that mimics the methodology of a clinical expert.\n",
        "\n",
        "The paper outlines two novel contributions to automatic AS detection:\n",
        "\n",
        "1) Supervised attention mechanism that identifies relevant TTE views (often unlabeled),\n",
        "mimicking human filtering done by a clinical expert. This is accomplished by introducing a\n",
        "new loss term, “supervised attention (SA)”, to match attention weights to the relevance\n",
        "scores from a View Relevance classifier.\n",
        "\n",
        "2) Self-supervised pretraining strategy through contrastive learning on the embedding of\n",
        "the entire TTE study (i.e., a “bag of images”) – compared to traditional pretraining\n",
        "strategies which focus on individual images.\n",
        "\n",
        "## Paper Results\n",
        "\n",
        "The paper uses *balanced accuracy* as the performance metric due to the class imbalance in the TMED-2 dataset -- making standard accuracy \"less suitable\" (Huang et al., 2023). The proposed method (SAMIL) was compared to general-purpose multi-instance algorithms and prior methods for AS diagnosis using deep neural networks.\n",
        "\n",
        "SAMIL performed much better (76% balanced accuracy) than 4 other state-of-the-art attention-based MIL architectures tested vs. a range of 60-67% balanced accuracy for existing algorithms.\n",
        "\n",
        "The chart below from the original paper outlines the balanced accuracy of SAMIL against other  approaches dedicated to AS diagnosis (*Filter then Average* and *Weighted Average by View Relevance*), and other general approaches including ABMIL, Set Transformer, and DSMIL.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1IpY4tUQAVBqCfwaPM73vmP8bVcFHnqe6)\n",
        "\n",
        "(Huang et al., 2023)"
      ],
      "metadata": {
        "id": "MQ0sNuMePBXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scope of Reproducibility:\n",
        "\n",
        "The scope of this project is to reproduce the original claims in the paper. Using the existing code provided by the authors of the paper, each model will be trained using the TMED-2 dataset and the paper's claimed Balanced Accuracy scores will be compared to the results of our training.\n",
        "\n",
        "While the original paper compares SAMIL to ABMIL and DSMIL, only ABMIL will be compared. Additionally, only Split 1 will be trained due to the computational requirements needed to train just a single split - training all three is not feasible at this time.\n",
        "\n",
        "### Hypotheses To Be Tested\n",
        "\n",
        "1. A supervised attention mechanism will provide significant improvements over\n",
        "standard MIL approaches in AS detection rates and detection accuracy, with a\n",
        "smaller model size.\n",
        "2. Self-supervised pretraining of “study-level” TTE artifacts provides improvements in\n",
        "AS detection rates and detection accuracy over traditional “image-level” pretraining,\n",
        "or no pretraining at all.\n",
        "\n",
        "### Planned Ablations\n",
        "\n",
        "The paper has two ablations targeting the attention strategy and the pretraining strategy.\n",
        "\n",
        "1. Attention: The attention mechanisms within the pooling layer σ to be tested are the\n",
        "baseline ABMIL model, ABMIL with gated attention, and the SAMIL model without\n",
        "pretraining. The paper compares the performance of these three approaches and identifies\n",
        "that SAMIL’s supervised attention model outperforms ABMIL (the baseline model that\n",
        "SAMIL builds upon) by +1200 bps. The Github repo scripts includes parameters to control\n",
        "the attention mechanism for ABMIL (gated_attention vs. attention), and SAMIL with and\n",
        "without pretraining.\n",
        "2. Pre Training: The paper introduces a novel approach of built-in study-level (i.e., bag-level)\n",
        "pretraining. This ablation compares different pretraining strategies including: image-level\n",
        "contrastive learning and no pretraining to the study-level pretraining approach. The paper\n",
        "finds no improvements with image-level pretraining, but the study-level pretraining shows\n",
        "improvements of +480 bps. The Github repo scripts include parameters to control\n",
        "pretraining options of: study level, image level, and none."
      ],
      "metadata": {
        "id": "uygL9tTPSVHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methodology\n",
        "\n",
        "To reproduce this paper, the following pre-requisites must be acquired:\n",
        "\n",
        "1. Access to the TMED-2 dataset [here](https://tmed.cs.tufts.edu/tmed_v2.html)\n",
        "2. Download the pretrained view classifiers, MOCO pretrained checkpoints, and training curves of SAMIL from the paper's Github repo [here](https://tufts.box.com/s/c5w8123j7h3dpls75jye1363uh8qv8us). Once downloaded, upload the entire unzipped folder to your Google Drive (see path below).\n",
        "\n",
        "The methodology for reproduction is as follows:\n",
        "\n",
        "1. Create and train the ABMIL model\n",
        "2. Create and train the SAMIL model with no Pretraining\n",
        "3. Train the SAMIL model with Image Level Pretraining\n",
        "4. Train the SAMIL model with Study Level Pretraining\n",
        "\n",
        "The model definitions and helper methods are pulled from the paper's Github repo."
      ],
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "from google.colab import drive\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Modify the paths below if they differ from your upload locations.\n",
        "# Assumes the SAMIL Github repo has been cloned and uploaded to drive in the `SAMIL` folder.\n",
        "MODEL_CHECKPOINTS = '/content/drive/MyDrive/SAMIL/model_checkpoints'\n",
        "ROOT_DIR = '/content/drive/MyDrive/SAMIL'\n",
        "DATA_INFO_DIR = '/content/drive/MyDrive/SAMIL/data_info'\n",
        "DATA_DIR = '/content/drive/MyDrive/DL4H-TMED2/'\n",
        "\n",
        "with zipfile.ZipFile(DATA_DIR + 'labeled.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/data')\n",
        "\n",
        "with zipfile.ZipFile(DATA_DIR + 'unlabeled.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/data')\n",
        "\n",
        "LOCAL_DATA_DIR = '/content/data/'"
      ],
      "metadata": {
        "id": "yu61Jp1xrnKk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b673bd5-df0c-4f43-b2ac-3fa8c0c0c989"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment Setup"
      ],
      "metadata": {
        "id": "IvVvLfNOL-xp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import glob\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "\n",
        "trainingSeed = 0\n",
        "batchSize = 1\n",
        "numWorkers = 8\n",
        "random.seed(trainingSeed)\n",
        "np.random.seed(trainingSeed)\n",
        "torch.manual_seed(trainingSeed)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "fqarUGilLUY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Data\n",
        "The dataset used by the paper is the[ TMED-2 dataset](https://tmed.cs.tufts.edu/tmed_v2.html), containing transthoracic echocardiogram (TTE) imagery from routine care of patients at Tufts Medical Center.\n",
        "\n",
        "The paper uses the (`view_and_diagnosis_labeled_set`) from TMED-2, consisting of 599 studies from 577 patients. The patients are labeled by board certified medical staff with the following values: none, early AS, or significant AS. The dataset has been partioned into different splits, each containing 360 training studies, 119 validation studies, and 120 test studies.\n",
        "\n",
        "Code blocks below should only be executed after you have acquired the TMED-2 dataset and uploaded the `view_and_diagnosis_labeled_set` folder to Drive."
      ],
      "metadata": {
        "id": "2NbPHUTMbkD3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZScZNbROw-N"
      },
      "outputs": [],
      "source": [
        "labeled_dir = '/content/drive/MyDrive/DL4H-TMED2/labeled'\n",
        "unlabeled_dir = '/content/drive/MyDrive/DL4H-TMED2/unlabeled'\n",
        "\n",
        "# Assumes the `view_and_diagnosis_labeled_set` is uploaded to a folder named `DL4H-TMED2`\n",
        "\n",
        "TMED2SummaryTable = pd.read_csv(os.path.join(DATA_INFO_DIR, 'TMED2SummaryTable.csv'))\n",
        "SEED_DIR = DATA_INFO_DIR + '/DataPartition/seed0/DEV479/FullyLabeledSet_studies'\n",
        "\n",
        "train_PatientStudy_list = pd.read_csv(os.path.join(SEED_DIR, \"train_studies.csv\"))\n",
        "val_PatientStudy_list = pd.read_csv(os.path.join(SEED_DIR, \"val_studies.csv\"))\n",
        "test_PatientStudy_list = pd.read_csv(os.path.join(SEED_DIR, \"test_studies.csv\"))\n",
        "\n",
        "train_PatientStudy_ids = train_PatientStudy_list[\"study\"].values\n",
        "val_PatientStudy_ids = val_PatientStudy_list[\"study\"].values\n",
        "test_PatientStudy_ids = test_PatientStudy_list[\"study\"].values\n",
        "\n",
        "#Debug\n",
        "#print(train_PatientStudy_ids)\n",
        "# print(val_PatientStudy_ids)\n",
        "# print(test_PatientStudy_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EchoDataset\n",
        "\n",
        "The class below is directly from the paper repo and handles transforming and loading the TMED-2 image data."
      ],
      "metadata": {
        "id": "KcfpY-xjPB6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "DiagnosisStr_to_Int_Mapping={\n",
        "    'no_AS':0,\n",
        "    'mild_AS':1,\n",
        "    'mildtomod_AS':1,\n",
        "    'moderate_AS':2,\n",
        "    'severe_AS':2\n",
        "\n",
        "}\n",
        "\n",
        "class EchoDataset(Dataset):\n",
        "    def __init__(self, PatientStudy_list, TMED2SummaryTable, ML_DATA_dir, sampling_strategy='first_frame', training_seed=0, transform_fn=None):\n",
        "\n",
        "        self.PatientStudy_list = PatientStudy_list\n",
        "        self.TMED2SummaryTable = TMED2SummaryTable #note: using the patient_id column in TMED2SummaryTable can uniquely identify a patient_study (there is NO same patient_study belong to different parts: diagnosis_labeled/, unlabeled/, view_and_diagnosis_labeled_set/, view_labeled AT THE SAME TIME)\n",
        "\n",
        "        self.ML_DATA_dir = ML_DATA_dir\n",
        "\n",
        "        self.sampling_strategy = sampling_strategy\n",
        "\n",
        "        self.training_seed=training_seed\n",
        "\n",
        "        self.transform_fn = transform_fn\n",
        "\n",
        "        self.bag_of_PiatentStudy_images, self.bag_of_PatientStudy_DiagnosisLabels = self._create_bags()\n",
        "\n",
        "\n",
        "\n",
        "    def _create_bags(self):\n",
        "\n",
        "        bag_of_PatientStudy_images = []\n",
        "        bag_of_PatientStudy_DiagnosisLabels = []\n",
        "\n",
        "        for PatientStudy in self.PatientStudy_list:\n",
        "            this_PatientStudyRecords_from_TMED2SummaryTable = self.TMED2SummaryTable[self.TMED2SummaryTable['patient_study']==PatientStudy]\n",
        "            assert this_PatientStudyRecords_from_TMED2SummaryTable.shape[0]!=0, 'every PatientStudy from the studylist should be found in TMED2SummaryTable'\n",
        "\n",
        "            this_PatientStudyRecords_from_TMED2SummaryTable_DiagnosisLabel = list(set(this_PatientStudyRecords_from_TMED2SummaryTable.diagnosis_label.values))\n",
        "            assert len(this_PatientStudyRecords_from_TMED2SummaryTable_DiagnosisLabel)==1, 'every PatientStudy should only have one diagnosis label'\n",
        "\n",
        "            this_PatientStudy_DiagnosisLabel = this_PatientStudyRecords_from_TMED2SummaryTable_DiagnosisLabel[0]\n",
        "            this_PatientStudy_DiagnosisLabel = DiagnosisStr_to_Int_Mapping[this_PatientStudy_DiagnosisLabel]\n",
        "\n",
        "            this_PatientStudy_Id_ImagesPattern = PatientStudy + \"_*.png\"\n",
        "            this_PatientStudy_Id_LabeledImages: list[str] = glob.glob(pathname=this_PatientStudy_Id_ImagesPattern, root_dir=os.path.join(self.ML_DATA_dir, \"labeled\"))\n",
        "            this_PatientStudy_Id_UnlabeledImages: list[str] = glob.glob(pathname=this_PatientStudy_Id_ImagesPattern, root_dir=os.path.join(self.ML_DATA_dir, \"unlabeled\"))\n",
        "\n",
        "            # From paper repo, sort to ensure order of images are consistent each run.\n",
        "            this_PatientStudy_Id_LabeledImages.sort()\n",
        "            this_PatientStudy_Id_UnlabeledImages.sort()\n",
        "\n",
        "            this_PatientStudyImages = []\n",
        "\n",
        "            for ImagePath in this_PatientStudy_Id_LabeledImages:\n",
        "                this_PatientStudyImages.append(\n",
        "                    np.array(Image.open(self.ML_DATA_dir + '/labeled/' + ImagePath).convert(mode=\"RGB\"))\n",
        "                )\n",
        "\n",
        "            for ImagePath in this_PatientStudy_Id_UnlabeledImages:\n",
        "                this_PatientStudyImages.append(\n",
        "                    np.array(Image.open(self.ML_DATA_dir + '/unlabeled/' + ImagePath).convert(mode=\"RGB\"))\n",
        "                )\n",
        "\n",
        "            bag_of_PatientStudy_images.append(np.array(this_PatientStudyImages))\n",
        "            bag_of_PatientStudy_DiagnosisLabels.append(this_PatientStudy_DiagnosisLabel)\n",
        "\n",
        "        return bag_of_PatientStudy_images, bag_of_PatientStudy_DiagnosisLabels\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.bag_of_PiatentStudy_images)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        bag_image = self.bag_of_PiatentStudy_images[index]\n",
        "\n",
        "        if self.transform_fn is not None:\n",
        "            bag_image = torch.stack([self.transform_fn(Image.fromarray(image)) for image in bag_image])\n",
        "\n",
        "\n",
        "        DiagnosisLabel = self.bag_of_PatientStudy_DiagnosisLabels[index]\n",
        "\n",
        "        return bag_image, DiagnosisLabel\n",
        "\n"
      ],
      "metadata": {
        "id": "SG95OSd6PBuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformations"
      ],
      "metadata": {
        "id": "eWNFwtCAQGpb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL\n",
        "import PIL.ImageOps\n",
        "import PIL.ImageEnhance\n",
        "import PIL.ImageDraw\n",
        "from PIL import Image\n",
        "\n",
        "PARAMETER_MAX = 10\n",
        "\n",
        "\n",
        "def AutoContrast(img, **kwarg):\n",
        "    return PIL.ImageOps.autocontrast(img)\n",
        "\n",
        "\n",
        "def Brightness(img, v, max_v, bias=0):\n",
        "    v = _float_parameter(v, max_v) + bias\n",
        "    return PIL.ImageEnhance.Brightness(img).enhance(v)\n",
        "\n",
        "\n",
        "def Color(img, v, max_v, bias=0):\n",
        "    v = _float_parameter(v, max_v) + bias\n",
        "    return PIL.ImageEnhance.Color(img).enhance(v)\n",
        "\n",
        "\n",
        "def Contrast(img, v, max_v, bias=0):\n",
        "    v = _float_parameter(v, max_v) + bias\n",
        "    return PIL.ImageEnhance.Contrast(img).enhance(v)\n",
        "\n",
        "\n",
        "def Cutout(img, v, max_v, bias=0):\n",
        "    if v == 0:\n",
        "        return img\n",
        "    v = _float_parameter(v, max_v) + bias\n",
        "    v = int(v * min(img.size))\n",
        "    return CutoutAbs(img, v)\n",
        "\n",
        "\n",
        "def CutoutAbs(img, v, **kwarg):\n",
        "    w, h = img.size\n",
        "    x0 = np.random.uniform(0, w)\n",
        "    y0 = np.random.uniform(0, h)\n",
        "    x0 = int(max(0, x0 - v / 2.))\n",
        "    y0 = int(max(0, y0 - v / 2.))\n",
        "    x1 = int(min(w, x0 + v))\n",
        "    y1 = int(min(h, y0 + v))\n",
        "    xy = (x0, y0, x1, y1)\n",
        "    # gray\n",
        "    color = (127, 127, 127)\n",
        "    img = img.copy()\n",
        "    PIL.ImageDraw.Draw(img).rectangle(xy, color)\n",
        "    return img\n",
        "\n",
        "\n",
        "def Equalize(img, **kwarg):\n",
        "    return PIL.ImageOps.equalize(img)\n",
        "\n",
        "\n",
        "def Identity(img, **kwarg):\n",
        "    return img\n",
        "\n",
        "\n",
        "def Invert(img, **kwarg):\n",
        "    return PIL.ImageOps.invert(img)\n",
        "\n",
        "\n",
        "def Posterize(img, v, max_v, bias=0):\n",
        "    v = _int_parameter(v, max_v) + bias\n",
        "    return PIL.ImageOps.posterize(img, v)\n",
        "\n",
        "\n",
        "def Rotate(img, v, max_v, bias=0):\n",
        "    v = _int_parameter(v, max_v) + bias\n",
        "    if random.random() < 0.5:\n",
        "        v = -v\n",
        "    return img.rotate(v)\n",
        "\n",
        "\n",
        "def Sharpness(img, v, max_v, bias=0):\n",
        "    v = _float_parameter(v, max_v) + bias\n",
        "    return PIL.ImageEnhance.Sharpness(img).enhance(v)\n",
        "\n",
        "\n",
        "def ShearX(img, v, max_v, bias=0):\n",
        "    v = _float_parameter(v, max_v) + bias\n",
        "    if random.random() < 0.5:\n",
        "        v = -v\n",
        "    return img.transform(img.size, PIL.Image.AFFINE, (1, v, 0, 0, 1, 0))\n",
        "\n",
        "\n",
        "def ShearY(img, v, max_v, bias=0):\n",
        "    v = _float_parameter(v, max_v) + bias\n",
        "    if random.random() < 0.5:\n",
        "        v = -v\n",
        "    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, v, 1, 0))\n",
        "\n",
        "\n",
        "def Solarize(img, v, max_v, bias=0):\n",
        "    v = _int_parameter(v, max_v) + bias\n",
        "    return PIL.ImageOps.solarize(img, 256 - v)\n",
        "\n",
        "\n",
        "def SolarizeAdd(img, v, max_v, bias=0, threshold=128):\n",
        "    v = _int_parameter(v, max_v) + bias\n",
        "    if random.random() < 0.5:\n",
        "        v = -v\n",
        "    img_np = np.array(img).astype(np.int)\n",
        "    img_np = img_np + v\n",
        "    img_np = np.clip(img_np, 0, 255)\n",
        "    img_np = img_np.astype(np.uint8)\n",
        "    img = Image.fromarray(img_np)\n",
        "    return PIL.ImageOps.solarize(img, threshold)\n",
        "\n",
        "\n",
        "def TranslateX(img, v, max_v, bias=0):\n",
        "    v = _float_parameter(v, max_v) + bias\n",
        "    if random.random() < 0.5:\n",
        "        v = -v\n",
        "    v = int(v * img.size[0])\n",
        "    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, v, 0, 1, 0))\n",
        "\n",
        "\n",
        "def TranslateY(img, v, max_v, bias=0):\n",
        "    v = _float_parameter(v, max_v) + bias\n",
        "    if random.random() < 0.5:\n",
        "        v = -v\n",
        "    v = int(v * img.size[1])\n",
        "    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, 0, 1, v))\n",
        "\n",
        "\n",
        "def _float_parameter(v, max_v):\n",
        "    return float(v) * max_v / PARAMETER_MAX\n",
        "\n",
        "\n",
        "def _int_parameter(v, max_v):\n",
        "    return int(v * max_v / PARAMETER_MAX)\n",
        "\n",
        "\n",
        "def fixmatch_augment_pool():\n",
        "    # FixMatch paper\n",
        "    augs = [(AutoContrast, None, None),\n",
        "            (Brightness, 0.9, 0.05),\n",
        "            (Color, 0.9, 0.05),\n",
        "            (Contrast, 0.9, 0.05),\n",
        "            (Equalize, None, None),\n",
        "            (Identity, None, None),\n",
        "            (Posterize, 4, 4),\n",
        "            (Rotate, 30, 0),\n",
        "            (Sharpness, 0.9, 0.05),\n",
        "            (ShearX, 0.3, 0),\n",
        "            (ShearY, 0.3, 0),\n",
        "            (Solarize, 256, 0),\n",
        "            (TranslateX, 0.3, 0),\n",
        "            (TranslateY, 0.3, 0)]\n",
        "    return augs\n",
        "\n",
        "class RandAugmentMC(object):\n",
        "    def __init__(self, n, m):\n",
        "        assert n >= 1\n",
        "        assert 1 <= m <= 10\n",
        "        self.n = n\n",
        "        self.m = m\n",
        "        self.augment_pool = fixmatch_augment_pool()\n",
        "\n",
        "    def __call__(self, img):\n",
        "        ops = random.choices(self.augment_pool, k=self.n)\n",
        "        for op, max_v, bias in ops:\n",
        "            v = np.random.randint(1, self.m)\n",
        "            if random.random() < 0.5:\n",
        "                img = op(img, v=v, max_v=max_v, bias=bias)\n",
        "        img = CutoutAbs(img, int(32*0.5))\n",
        "        return img\n"
      ],
      "metadata": {
        "id": "HRtKgvAfQilN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform_eval = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "transform_labeledtrain = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(size=112,\n",
        "                          padding=int(112*0.125),\n",
        "                          padding_mode='reflect'),\n",
        "    RandAugmentMC(n=2, m=10),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ],
      "metadata": {
        "id": "J6DKaEXcyre2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Dataset\n",
        "\n",
        "The following code blocks preprocess the data and load the data using a modified implementation from the original paper to handle the TMED-2 dataset structure."
      ],
      "metadata": {
        "id": "0OX4ayzyyujv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = EchoDataset(train_PatientStudy_ids, TMED2SummaryTable, LOCAL_DATA_DIR, sampling_strategy='first_frame', training_seed=trainingSeed, transform_fn=transform_labeledtrain)\n",
        "\n",
        "trainmemory_dataset = EchoDataset(train_PatientStudy_ids, TMED2SummaryTable, LOCAL_DATA_DIR, sampling_strategy='first_frame', training_seed=trainingSeed, transform_fn=transform_eval)\n",
        "\n",
        "val_dataset = EchoDataset(val_PatientStudy_ids, TMED2SummaryTable, LOCAL_DATA_DIR, sampling_strategy='first_frame', training_seed=trainingSeed, transform_fn=transform_eval)\n",
        "\n",
        "test_dataset = EchoDataset(test_PatientStudy_ids, TMED2SummaryTable, LOCAL_DATA_DIR, sampling_strategy='first_frame', training_seed=trainingSeed, transform_fn=transform_eval)"
      ],
      "metadata": {
        "id": "-IGAOpzcQ7tU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"train: {}, trainmemory: {}, val: {}, test: {}\".format(len(train_dataset), len(trainmemory_dataset), len(val_dataset), len(test_dataset)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNcTI_1Nyo59",
        "outputId": "0f1e4c27-b503-4b1d-d6a4-cd25b534332e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: 360, trainmemory: 360, val: 119, test: 120\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PAinFF1jfQRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=batchSize, shuffle=True, num_workers=numWorkers)\n",
        "trainmemory_loader = DataLoader(trainmemory_dataset, batch_size=batchSize, shuffle=False, num_workers=numWorkers)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batchSize, shuffle=False, num_workers=numWorkers)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batchSize, shuffle=False, num_workers=numWorkers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdJ2wTPKzAMS",
        "outputId": "aae5b1b3-3531-473b-c8fc-277cea6beb9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Analysis\n",
        "\n",
        "The TMED-2 `view_and_diagnosis_labeled_set` has 3 labels: `no_AS` which maps to `0`, `mod_AS` which maps to `1`, and `sev_AS` which maps to 2. In simple terms, these labels stand for No AS diagnosis, Moderate AS diagnosis, and Severe AS diagnosis.\n",
        "\n",
        "From the dataset statistics method, we find the dataset to be imbalanced towards positive diagnoses of AS, namely Severe AS. There is risk in the trained models biasing towards a diagnosis vs. no AS diagnosis.\n"
      ],
      "metadata": {
        "id": "V88ygvXO0cKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_statistics(loader, name):\n",
        "    num_batches = len(loader)\n",
        "    num_samples = len(loader.dataset)\n",
        "    label_mapping = {0: \"no_AS\", 1: \"mod_AS\", 2: \"sev_AS\"}\n",
        "\n",
        "    print(f\"{name} DataLoader:\")\n",
        "    print(f\"  Total number of batches: {num_batches}\")\n",
        "    print(f\"  Total number of samples: {num_samples}\")\n",
        "\n",
        "    labels = []\n",
        "    for _, batch_labels in loader:\n",
        "        labels.extend(batch_labels.tolist())\n",
        "    num_classes = len(set(labels))\n",
        "    print(f\"  Number of classes: {num_classes}\")\n",
        "\n",
        "    class_distribution = {label_mapping[label]: labels.count(label) for label in set(labels) if label in label_mapping}\n",
        "    print(f\"  Class distribution: {class_distribution}\")\n",
        "\n",
        "dataset_statistics(train_loader, \"Train\")\n",
        "dataset_statistics(trainmemory_loader, \"Train Memory\")\n",
        "dataset_statistics(val_loader, \"Validation\")\n",
        "dataset_statistics(test_loader, \"Test\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5rI2m2w0Cwy",
        "outputId": "19bfc36b-ab18-4a63-ffaf-e45f27aff24b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train DataLoader:\n",
            "  Total number of batches: 360\n",
            "  Total number of samples: 360\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Number of classes: 3\n",
            "  Class distribution: {'no_AS': 76, 'mod_AS': 103, 'sev_AS': 181}\n",
            "Train Memory DataLoader:\n",
            "  Total number of batches: 360\n",
            "  Total number of samples: 360\n",
            "  Number of classes: 3\n",
            "  Class distribution: {'no_AS': 76, 'mod_AS': 103, 'sev_AS': 181}\n",
            "Validation DataLoader:\n",
            "  Total number of batches: 119\n",
            "  Total number of samples: 119\n",
            "  Number of classes: 3\n",
            "  Class distribution: {'no_AS': 25, 'mod_AS': 34, 'sev_AS': 60}\n",
            "Test DataLoader:\n",
            "  Total number of batches: 120\n",
            "  Total number of samples: 120\n",
            "  Number of classes: 3\n",
            "  Class distribution: {'no_AS': 26, 'mod_AS': 34, 'sev_AS': 60}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##   Model\n",
        "\n",
        "The paper evaluates multiple models in addition to the SAMIL model they have contributed. These models include ABMIL and DSMIL. As mentioned previously, only ABMIL and SAMIL will be trained in this reproduction.\n",
        "\n",
        "### SAMIL Model\n",
        "\n",
        "#### View Classifier\n",
        "\n",
        "One of the novel contributions of the SAMIL paper is the introduction of supervised attention MIL (SAMIL). This is by introducing a \"view-type relevance clasifier\" to only pay attention to specific views of echo imagery. This so called \"View Classifier\" is a separately trained model based on the WideResNet architecture.\n",
        "\n",
        "The following class for the View Classifier is directly ported from the paper's Github repo."
      ],
      "metadata": {
        "id": "3muyDPFPbozY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import sys\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s | %(levelname)s : %(message)s',\n",
        "                     level=logging.INFO, stream=sys.stdout)\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "\n",
        "\n",
        "def mish(x):\n",
        "    \"\"\"Mish: A Self Regularized Non-Monotonic Neural Activation Function (https://arxiv.org/abs/1908.08681)\"\"\"\n",
        "    return x * torch.tanh(F.softplus(x))\n",
        "\n",
        "\n",
        "class PSBatchNorm2d(nn.BatchNorm2d):\n",
        "    \"\"\"How Does BN Increase Collapsed Neural Network Filters? (https://arxiv.org/abs/2001.11216)\"\"\"\n",
        "\n",
        "    def __init__(self, num_features, alpha=0.1, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True):\n",
        "        super().__init__(num_features, eps, momentum, affine, track_running_stats)\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, x):\n",
        "        return super().forward(x) + self.alpha\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, stride, drop_rate=0.0, activate_before_residual=False):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes, momentum=0.001)\n",
        "        self.relu1 = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
        "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_planes, momentum=0.001)\n",
        "        self.relu2 = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        self.drop_rate = drop_rate\n",
        "        self.equalInOut = (in_planes == out_planes)\n",
        "        self.convShortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n",
        "                                                                padding=0, bias=False) or None\n",
        "        self.activate_before_residual = activate_before_residual\n",
        "\n",
        "    def forward(self, x):\n",
        "        if not self.equalInOut and self.activate_before_residual == True:\n",
        "            x = self.relu1(self.bn1(x))\n",
        "        else:\n",
        "            out = self.relu1(self.bn1(x))\n",
        "        out = self.relu2(self.bn2(self.conv1(out if self.equalInOut else x)))\n",
        "        if self.drop_rate > 0:\n",
        "            out = F.dropout(out, p=self.drop_rate, training=self.training)\n",
        "        out = self.conv2(out)\n",
        "        return torch.add(x if self.equalInOut else self.convShortcut(x), out)\n",
        "\n",
        "\n",
        "class NetworkBlock(nn.Module):\n",
        "    def __init__(self, nb_layers, in_planes, out_planes, block, stride, drop_rate=0.0, activate_before_residual=False):\n",
        "        super(NetworkBlock, self).__init__()\n",
        "        self.layer = self._make_layer(\n",
        "            block, in_planes, out_planes, nb_layers, stride, drop_rate, activate_before_residual)\n",
        "\n",
        "    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, drop_rate, activate_before_residual):\n",
        "        layers = []\n",
        "        for i in range(int(nb_layers)):\n",
        "            layers.append(block(i == 0 and in_planes or out_planes, out_planes,\n",
        "                                i == 0 and stride or 1, drop_rate, activate_before_residual))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer(x)\n",
        "\n",
        "# args.model_depth = 28\n",
        "# args.model_width = 2\n",
        "\n",
        "class WideResNet(nn.Module):\n",
        "    def __init__(self, num_classes, depth=28, widen_factor=2, drop_rate=0.0):\n",
        "        super(WideResNet, self).__init__()\n",
        "        channels = [16, 16*widen_factor, 32*widen_factor, 64*widen_factor, 128*widen_factor]\n",
        "        assert((depth - 4) % 6 == 0)\n",
        "        n = (depth - 4) / 6 #equivalent to 'repeat' in tf repo\n",
        "        block = BasicBlock\n",
        "        # 1st conv before any network block\n",
        "        self.conv1 = nn.Conv2d(3, channels[0], kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        # 1st block\n",
        "        self.block1 = NetworkBlock(\n",
        "            n, channels[0], channels[1], block, 1, drop_rate, activate_before_residual=True)\n",
        "        # 2nd block\n",
        "        self.block2 = NetworkBlock(\n",
        "            n, channels[1], channels[2], block, 2, drop_rate)\n",
        "        # 3rd block\n",
        "        self.block3 = NetworkBlock(\n",
        "            n, channels[2], channels[3], block, 2, drop_rate)\n",
        "\n",
        "        # 4th block (hz added)\n",
        "        self.block4 = NetworkBlock(\n",
        "            n, channels[3], channels[4], block, 2, drop_rate)\n",
        "\n",
        "        # global average pooling and classifier\n",
        "        self.bn1 = nn.BatchNorm2d(channels[4], momentum=0.001)\n",
        "        self.relu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
        "        self.fc = nn.Linear(channels[4], num_classes)\n",
        "        self.channels = channels[4]\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight,\n",
        "                                        mode='fan_out',\n",
        "                                        nonlinearity='leaky_relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1.0)\n",
        "                nn.init.constant_(m.bias, 0.0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal_(m.weight)\n",
        "                nn.init.constant_(m.bias, 0.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.block1(out)\n",
        "        out = self.block2(out)\n",
        "        out = self.block3(out)\n",
        "        out = self.block4(out)\n",
        "\n",
        "        out = self.relu(self.bn1(out))\n",
        "        out = F.adaptive_avg_pool2d(out, 1)\n",
        "        out = out.view(-1, self.channels)\n",
        "        return self.fc(out)\n",
        "\n",
        "\n",
        "def build_wideresnet(depth, widen_factor, dropout, num_classes):\n",
        "    logger.info(f\"Model: WideResNet {depth}x{widen_factor}\")\n",
        "    return WideResNet(depth=depth,\n",
        "                      widen_factor=widen_factor,\n",
        "                      drop_rate=dropout,\n",
        "                      num_classes=num_classes)\n"
      ],
      "metadata": {
        "id": "9XXrR6CnFrH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SAMIL Model\n",
        "\n",
        "The following class represents the SAMIL model. This code is unchanged from the paper's Github repo.\n",
        "\n",
        "The SAMIL model consist of a 3 layer, sequential Feature Extractor to generate an aggregated bag-level embedding from an embedding of each instance. Then two sequential attention layers steer focus towards relevant echo views, and lastly a classifier layer."
      ],
      "metadata": {
        "id": "5NNgGgv4Fy5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SAMIL(nn.Module):\n",
        "    def __init__(self, num_classes=3):\n",
        "        super(SAMIL, self).__init__()\n",
        "        self.L = 500\n",
        "        self.B = 250\n",
        "        self.D = 128\n",
        "        self.K = 1\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.feature_extractor_part1 = nn.Sequential(\n",
        "#             nn.Conv2d(1, 20, kernel_size=5),\n",
        "            nn.Conv2d(3, 20, kernel_size=5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.Conv2d(20, 50, kernel_size=5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "             #hz added\n",
        "            nn.Conv2d(50, 100, kernel_size=5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.Conv2d(100, 200, kernel_size=3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "        )\n",
        "\n",
        "        self.feature_extractor_part2 = nn.Sequential(\n",
        "#             nn.Linear(50 * 4 * 4, self.L),\n",
        "            nn.Linear(200 * 4 * 4, self.L),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.feature_extractor_part3 = nn.Sequential(\n",
        "\n",
        "            nn.Linear(self.L, self.B),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.B, self.L),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.attention_V = nn.Sequential(\n",
        "            nn.Linear(self.L, self.D),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(self.D, self.K)\n",
        "        )\n",
        "\n",
        "        self.attention_U = nn.Sequential(\n",
        "            nn.Linear(self.L, self.D),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(self.D, self.K)\n",
        "        )\n",
        "\n",
        "#         self.attention_weights = nn.Linear(self.D, self.K)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "#             nn.Linear(self.L*self.K, 1),\n",
        "            nn.Linear(self.L*self.K, self.num_classes),\n",
        "#             nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "#         print('Inside forward: input x shape: {}'.format(x.shape))\n",
        "        x = x.squeeze(0)\n",
        "#         print('Inside forward: after squeeze x shape: {}'.format(x.shape))\n",
        "\n",
        "        H = self.feature_extractor_part1(x)\n",
        "#         print('Inside forward: after feature_extractor_part1 H shape: {}'.format(H.shape))\n",
        "\n",
        "\n",
        "#         H = H.view(-1, 50 * 4 * 4)\n",
        "        H = H.view(-1, 200 * 4 * 4)\n",
        "#         print('Inside forward: after view H shape: {}'.format(H.shape))\n",
        "\n",
        "        H = self.feature_extractor_part2(H)  # NxL\n",
        "#         print('Inside forward: after feature_extractor_part2 H shape: {}'.format(H.shape))\n",
        "\n",
        "        A_V = self.attention_V(H)  # NxK\n",
        "#         print('Inside forward: A_V is {}, shape: {}'.format(A_V, A_V.shape))\n",
        "\n",
        "        A_V = torch.transpose(A_V, 1, 0)  # KxN\n",
        "#         print('Inside forward: A_V is {}, shape: {}'.format(A_V, A_V.shape))\n",
        "\n",
        "        A_V = F.softmax(A_V, dim=1)  # softmax over N\n",
        "#         print('Inside forward: A_V (View) is {}, shape: {}'.format(A_V, A_V.shape))\n",
        "\n",
        "\n",
        "        H = self.feature_extractor_part3(H)\n",
        "\n",
        "        A_U = self.attention_U(H)  # NxK\n",
        "#         print('Inside forward: A_U is {}, shape: {}'.format(A_U, A_U.shape))\n",
        "\n",
        "        A_U = torch.transpose(A_U, 1, 0)  # KxN\n",
        "#         print('Inside forward: A_U is {}, shape: {}'.format(A_U, A_U.shape))\n",
        "\n",
        "        A_U = F.softmax(A_U, dim=1)  # softmax over N\n",
        "#         print('Inside forward: A_U (Diagnosis) is {}, shape: {}'.format(A_U, A_U.shape))\n",
        "\n",
        "#         A = A_V * A_U\n",
        "#         print('Inside forward: final A is {}, shape: {}'.format(A, A.shape))\n",
        "        A = torch.exp(torch.log(A_V) + torch.log(A_U)) #numerically more stable?\n",
        "\n",
        "        A = A/torch.sum(A)\n",
        "#         A = F.softmax(A, dim=1)\n",
        "#         print('Inside forward: final A is {}, shape: {}'.format(A, A.shape))\n",
        "#         A = self.attention_weights(A_V * A_U) # element wise multiplication # NxK\n",
        "#         print('Inside forward: A is {}, shape: {}'.format(A, A.shape))\n",
        "\n",
        "#         A = torch.transpose(A, 1, 0)  # KxN\n",
        "# #         print('Inside forward: A is {}, shape: {}'.format(A, A.shape))\n",
        "\n",
        "#         A = F.softmax(A, dim=1)  # softmax over N\n",
        "# #         print('Inside forward: A is {}, shape: {}'.format(A, A.shape))\n",
        "\n",
        "        M = torch.mm(A, H)  # KxL #M can be regarded as final representation of this bag\n",
        "#         print('Inside forward: M is {}, shape: {}'.format(M, M.shape))\n",
        "\n",
        "        out = self.classifier(M)\n",
        "\n",
        "\n",
        "        return out, A_V #only view regularize one branch of the attention weights\n",
        "\n"
      ],
      "metadata": {
        "id": "K3AYt5fpFyUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SAMIL Helpers\n",
        "The following helper methods are from the paper's Github repo. Specifically the `src/SAMIL/main.py` file.\n",
        "\n"
      ],
      "metadata": {
        "id": "GdtMXCIsGQ6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def str2bool(s):\n",
        "    if s == 'True':\n",
        "        return True\n",
        "    elif s == 'False':\n",
        "        return False\n",
        "    else:\n",
        "        raise NameError('Bad string')\n",
        "\n",
        "def save_checkpoint(state, checkpoint_dir, filename='last_checkpoint.pth.tar'):\n",
        "    '''last_checkpoint.pth.tar or xxx_model_best.pth.tar'''\n",
        "\n",
        "    filepath = os.path.join(checkpoint_dir, filename)\n",
        "    torch.save(state, filepath)\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "def get_cosine_schedule_with_warmup(optimizer,\n",
        "                                    lr_warmup_epochs,\n",
        "                                    lr_cycle_epochs, #total train epochs\n",
        "                                    num_cycles=7./16.,\n",
        "                                    last_epoch=-1):\n",
        "    def _lr_lambda(current_epoch):\n",
        "        if current_epoch < lr_warmup_epochs:\n",
        "            return float(current_epoch) / float(max(1, lr_warmup_epochs))\n",
        "#         no_progress = float(current_epoch - lr_warmup_epochs) / \\\n",
        "#             float(max(1, float(lr_cycle_epochs) - lr_warmup_epochs))\n",
        "\n",
        "        #see if using restart\n",
        "        ###############################################################\n",
        "        if current_epoch%lr_cycle_epochs==0:\n",
        "            current_cycle_epoch=lr_cycle_epochs\n",
        "        else:\n",
        "            current_cycle_epoch = current_epoch%lr_cycle_epochs\n",
        "\n",
        "        no_progress = float(current_cycle_epoch - lr_warmup_epochs) / \\\n",
        "            float(max(1, float(lr_cycle_epochs) - lr_warmup_epochs))\n",
        "        #################################################################\n",
        "\n",
        "        return max(0., math.cos(math.pi * num_cycles * no_progress))\n",
        "\n",
        "    return LambdaLR(optimizer, _lr_lambda, last_epoch)\n",
        "\n",
        "def get_fixed_lr(optimizer,\n",
        "                lr_warmup_epochs,\n",
        "                lr_cycle_epochs, #total train iterations\n",
        "                num_cycles=7./16.,\n",
        "                last_epoch=-1):\n",
        "    def _lr_lambda(current_epoch):\n",
        "\n",
        "        return 1.0\n",
        "\n",
        "    return LambdaLR(optimizer, _lr_lambda, last_epoch)\n",
        "\n",
        "def create_view_model(args):\n",
        "\n",
        "    view_model = build_wideresnet(depth=28,\n",
        "                                        widen_factor=2,\n",
        "                                        dropout=0.0,\n",
        "                                        num_classes=3)\n",
        "\n",
        "    logger.info(\"Total params for View Model: {:.2f}M\".format(\n",
        "        sum(p.numel() for p in view_model.parameters())/1e6))\n",
        "\n",
        "\n",
        "    #load the saved checkpoint\n",
        "    if args['data_seed']==0:\n",
        "        args['view_checkpoint_path'] = os.path.join(args['checkpoint_dir'], 'view_classifier', 'seed0_model_best.pth.tar')\n",
        "    elif args['data_seed']==1:\n",
        "        args['view_checkpoint_path'] = os.path.join(args['checkpoint_dir'], 'view_classifier', 'seed1_model_best.pth.tar')\n",
        "    elif args['data_seed']==2:\n",
        "        args['view_checkpoint_path'] = os.path.join(args['checkpoint_dir'], 'view_classifier', 'seed2_model_best.pth.tar')\n",
        "    else:\n",
        "        raise NameError('?')\n",
        "\n",
        "\n",
        "    view_checkpoint = torch.load(args['view_checkpoint_path'], map_location=device)\n",
        "\n",
        "    view_model.load_state_dict(view_checkpoint['ema_state_dict'])\n",
        "\n",
        "    view_model.eval()\n",
        "\n",
        "    return view_model\n",
        "\n",
        "def create_model(args):\n",
        "    model = SAMIL()\n",
        "\n",
        "    if args['MIL_checkpoint_path'] !='':\n",
        "        print('!!!!!!!!!!!!!!!!!!!!!initializing from pretrained checkpoint!!!!!!!!!!!!!!!!!!!!!')\n",
        "        pretrained_dict = torch.load(args['MIL_checkpoint_path'], map_location=device)\n",
        "\n",
        "        #https://discuss.pytorch.org/t/dataparallel-changes-parameter-names-issue-with-load-state-dict/60211\n",
        "        #rename tensor in the pretrained dict\n",
        "        from collections import OrderedDict\n",
        "        new_state_dict = OrderedDict()\n",
        "        for k, v in pretrained_dict.items():\n",
        "#                         print(k)\n",
        "            if 'encoder_q' in k:\n",
        "#                             print('!extract: {}'.format(k))\n",
        "                name = '.'.join(k.split('.')[1:])\n",
        "#                             print('new_name: {}'.format(name))\n",
        "                new_state_dict[name] = v\n",
        "\n",
        "        model_dict = model.state_dict()\n",
        "\n",
        "        new_state_dict = {k: v for k, v in new_state_dict.items() if k in model_dict}\n",
        "        model_dict.update(new_state_dict)\n",
        "\n",
        "        # 3. load the new state dict\n",
        "        model.load_state_dict(model_dict)\n",
        "\n",
        "\n",
        "\n",
        "    logger.info(\"Total params: {:.2f}M\".format(\n",
        "        sum(p.numel() for p in model.parameters() if p.requires_grad)/1e6))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "BajOibMWGHCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SAMIL Training\n",
        "\n",
        "The method below sets up various arguments around pretraining. The paper explores three methods of training: No Pretraining, pre training the Feature Extrator (to learn instance-level representations), and pre training the study-level representations of all *K* images in a routine echocardiogram."
      ],
      "metadata": {
        "id": "NAzUDOb4MYRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "    level=logging.INFO\n",
        ")\n",
        "\n",
        "def setup_samil_train(args):\n",
        "\n",
        "    if args['training_seed'] is not None:\n",
        "        print('setting training seed{}'.format(args['training_seed']), flush=True)\n",
        "        set_seed(args['training_seed'])\n",
        "\n",
        "\n",
        "    if args['Pretrained'] == 'Whole':\n",
        "\n",
        "        if args['data_seed']==0:\n",
        "            args['MIL_checkpoint_path'] = os.path.join(args['checkpoint_dir'],'MOCO_Pretraining_StudyLevel', 'seed0_checkpoint.pt')\n",
        "        elif args['data_seed']==1:\n",
        "            args['MIL_checkpoint_path'] = os.path.join(args['checkpoint_dir'],'MOCO_Pretraining_StudyLevel', 'seed1_checkpoint.pt')\n",
        "        elif args['data_seed']==2:\n",
        "            args['MIL_checkpoint_path'] = os.path.join(args['checkpoint_dir'],'MOCO_Pretraining_StudyLevel', 'seed2_checkpoint.pt')\n",
        "        else:\n",
        "            raise NameError('NOT VALID PRETRAINED MODEL')\n",
        "\n",
        "    elif args['Pretrained'] == 'FeatureExtractor1':\n",
        "\n",
        "        if args['data_seed']==0:\n",
        "            args['MIL_checkpoint_path']=os.path.join(args['checkpoint_dir'], 'MOCO_Pretraining_ImageLevel', 'seed0_checkpoint.pt')\n",
        "        elif args['data_seed']==1:\n",
        "            args['MIL_checkpoint_path']=os.path.join(args['checkpoint_dir'], 'MOCO_Pretraining_ImageLevel', 'seed1_checkpoint.pt')\n",
        "        elif args['data_seed']==2:\n",
        "            args['MIL_checkpoint_path']=os.path.join(args['checkpoint_dir'], 'MOCO_Pretraining_ImageLevel', 'seed2_checkpoint.pt')\n",
        "        else:\n",
        "            raise NameError('NOT VALID PRETRAINED MODEL')\n",
        "\n",
        "\n",
        "    elif args['Pretrained'] == 'NoPretrain':\n",
        "        args['MIL_checkpoint_path']=''\n",
        "\n",
        "    else:\n",
        "        raise NameError('invalid pretrain option')\n",
        "\n",
        "    if args['use_class_weights'] == 'True':\n",
        "        print('!!!!!!!!Using pre-calculated class weights!!!!!!!!')\n",
        "\n",
        "        #indeed, every split should have the same class weight for diagnosis by our dataset construction\n",
        "        if args['data_seed'] == 0 and args['development_size'] == 'DEV479':\n",
        "            args['class_weights'] = '0.463,0.342,0.195'\n",
        "        elif args['data_seed'] == 1 and args['development_size'] == 'DEV479':\n",
        "            args['class_weights'] = '0.463,0.342,0.195'\n",
        "        elif args['data_seed'] == 2 and args['development_size'] == 'DEV479':\n",
        "            args['class_weights'] = '0.463,0.342,0.195'\n",
        "        else:\n",
        "            raise NameError('not valid class weights setting')\n",
        "\n",
        "    else:\n",
        "        args['class_weights'] = '1.0,1.0,1.0'\n",
        "        print('?????????Not using pre-calculated class weights?????????')\n",
        "\n",
        "    experiment_name = \"{}\".format(args['Pretrained'])\n",
        "\n",
        "    args['experiment_dir'] = os.path.join(args['train_dir'], experiment_name)\n",
        "\n",
        "    if args['resume'] != 'None':\n",
        "        args['resume_checkpoint_fullpath'] = os.path.join(args['experiment_dir'], args['resume'])\n",
        "        print('args.resume_checkpoint_fullpath: {}'.format(args['resume_checkpoint_fullpath']))\n",
        "    else:\n",
        "        args['resume_checkpoint_fullpath'] = None\n",
        "\n",
        "    os.makedirs(args['experiment_dir'], exist_ok=True)\n",
        "    args['writer'] = SummaryWriter(args['experiment_dir'])\n",
        "\n",
        "    brief_summary = {}\n",
        "    brief_summary['val_progression_view'] = {}\n",
        "\n",
        "    brief_summary['dataset_name'] = args['dataset_name']\n",
        "    brief_summary['algorithm'] = 'Echo_MIL'\n",
        "    brief_summary['hyperparameters'] = {\n",
        "        'train_epoch': args['train_epoch'],\n",
        "        'optimizer': args['optimizer_type'],\n",
        "        'lr': args['lr'],\n",
        "        'wd': args['wd'],\n",
        "        'T':args['T'],\n",
        "        'lambda_ViewRegularization':args['lambda_ViewRegularization']\n",
        "    }\n",
        "\n",
        "    return args, brief_summary"
      ],
      "metadata": {
        "id": "h9f7LMvfHw2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train One Epoch and Early Stop Logic\n",
        "\n",
        "The code block below contains the methods to train a single epoch and the early stop logic."
      ],
      "metadata": {
        "id": "MRrbLx33x3Jl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "\n",
        "import torch\n",
        "\n",
        "class ModelEMA(object):\n",
        "    def __init__(self, args, model, decay):\n",
        "        self.ema = deepcopy(model)\n",
        "        self.ema.to(args['device'])\n",
        "        self.ema.eval()\n",
        "        self.decay = decay\n",
        "        self.ema_has_module = hasattr(self.ema, 'module')\n",
        "        # Fix EMA. https://github.com/valencebond/FixMatch_pytorch thank you!\n",
        "        self.param_keys = [k for k, _ in self.ema.named_parameters()]\n",
        "        self.buffer_keys = [k for k, _ in self.ema.named_buffers()]\n",
        "\n",
        "        print('self.param_keys: {}'.format(self.param_keys))\n",
        "        print('self.buffer_keys: {}'.format(self.buffer_keys))\n",
        "\n",
        "        for p in self.ema.parameters():\n",
        "#             print('Inside ModelEMA, p dtype is {}'.format(p.dtype))\n",
        "            p.requires_grad_(False)\n",
        "\n",
        "\n",
        "    def update(self, model):\n",
        "        needs_module = hasattr(model, 'module') and not self.ema_has_module\n",
        "        with torch.no_grad():\n",
        "            msd = model.state_dict()\n",
        "            esd = self.ema.state_dict()\n",
        "            for k in self.param_keys:\n",
        "                if needs_module:\n",
        "                    j = 'module.' + k\n",
        "                else:\n",
        "                    j = k\n",
        "                model_v = msd[j].detach()\n",
        "                ema_v = esd[k]\n",
        "                esd[k].copy_(ema_v * self.decay + (1. - self.decay) * model_v)\n",
        "\n",
        "            for k in self.buffer_keys:\n",
        "                if needs_module:\n",
        "                    j = 'module.' + k\n",
        "                else:\n",
        "                    j = k\n",
        "                esd[k].copy_(msd[j])\n",
        "\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import logging\n",
        "from sklearn.metrics import confusion_matrix as sklearn_cm\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import confusion_matrix as sklearn_cm\n",
        "\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation acc doesn't improve after a given patience.\"\"\"\n",
        "\n",
        "    def __init__(self, patience=300, initial_count=0, delta=0):\n",
        "\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 20\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        self.patience = patience\n",
        "        self.counter = initial_count\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.delta = delta\n",
        "\n",
        "\n",
        "    def __call__(self, val_acc):\n",
        "\n",
        "        score = val_acc\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "\n",
        "        elif score <= self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.counter = 0\n",
        "\n",
        "        print('!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!counter: {}, score: {}, best_score: {}'.format(self.counter, score, self.best_score))\n",
        "\n",
        "        return self.counter\n",
        "\n",
        "\n",
        "def train_one_epoch(args, weights, train_loader, model, ema_model, view_model, optimizer, scheduler, epoch):\n",
        "\n",
        "    args['writer'].add_scalar('train/lr', scheduler.get_last_lr()[0], epoch)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    TotalLoss_this_epoch, LabeledCELoss_this_epoch, ViewRegularizationLoss_this_epoch, scaled_ViewRegularizationLoss_this_epoch = [], [], [], []\n",
        "\n",
        "    train_iter = iter(train_loader)\n",
        "    n_steps_per_epoch = 360 #360 train studies, batch size 1\n",
        "    p_bar = tqdm(range(n_steps_per_epoch), disable=False)\n",
        "\n",
        "#     for batch_idx, (data, bag_label, view_relevance) in enumerate(tqdm(train_loader)):\n",
        "    for batch_idx in range(n_steps_per_epoch):\n",
        "\n",
        "        try:\n",
        "            data, bag_label = next(train_iter)\n",
        "        except:\n",
        "            train_iter = iter(train_loader)\n",
        "            data, bag_label = next(train_iter)\n",
        "\n",
        "#         print('batch_idx: {}'.format(batch_idx))\n",
        "\n",
        "#         print('type(data): {}, data.size: {}, require grad: {}'.format(type(data), data.size(), data.requires_grad))\n",
        "#         print('type(bag_label): {}, bag_label: {}'.format(type(bag_label), bag_label))\n",
        "#         print('type(view_relevance): {}, view_relevance: {}'.format(type(view_relevance), view_relevance))\n",
        "        data, bag_label = data.to(args['device']), bag_label.to(args['device'])\n",
        "\n",
        "\n",
        "        outputs, attentions = model(data)\n",
        "\n",
        "        log_attentions = torch.log(attentions)\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            view_predictions = view_model(data.squeeze(0))\n",
        "            softmax_view_predictions = F.softmax(view_predictions, dim=1)\n",
        "            predicted_relevance = softmax_view_predictions[:, :2]\n",
        "            predicted_relevance = torch.sum(predicted_relevance, dim=1)\n",
        "            predicted_relative_relevance = F.softmax(predicted_relevance/args['T'])\n",
        "            predicted_relative_relevance = predicted_relative_relevance.unsqueeze(0)\n",
        "\n",
        "\n",
        "        #element shape in F.cross_entropy: prediction torch.size([batch_size, num_classes]) and true label torch.size([batch_size])\n",
        "        if args['use_class_weights'] == 'True':\n",
        "            LabeledCELoss = F.cross_entropy(outputs, bag_label, weights, reduction='mean')\n",
        "        else:\n",
        "            LabeledCELoss = F.cross_entropy(outputs, bag_label, reduction='mean')\n",
        "\n",
        "\n",
        "#         parser.add_argument('--ViewRegularization_warmup_pos', default=0.4, type=float, help='position at which view regularization loss warmup ends') #following MixMatch and FixMatch repo\n",
        "\n",
        "# parser.add_argument('--ViewRegularization_warmup_schedule_type', default='NoWarmup', choices=['NoWarmup', 'Linear', 'Sigmoid', ], type=str)\n",
        "\n",
        "        #ViewRegularization warmup schedule choice\n",
        "        if args['ViewRegularization_warmup_schedule_type'] == 'NoWarmup':\n",
        "            current_warmup = 1\n",
        "        elif args['ViewRegularization_warmup_schedule_type'] == 'Linear':\n",
        "            current_warmup = np.clip(epoch/(float(args['ViewRegularization_warmup_pos']) * args['train_epoch']), 0, 1)\n",
        "        elif args['ViewRegularization_warmup_schedule_type'] == 'Sigmoid':\n",
        "            current_warmup = math.exp(-5 * (1 - min(epoch/(float(args['ViewRegularization_warmup_pos']) * args['train_epoch']), 1))**2)\n",
        "        else:\n",
        "            raise NameError('Not supported ViewRegularization warmup schedule')\n",
        "\n",
        "\n",
        "\n",
        "        ViewRegularizationLoss = F.kl_div(input=log_attentions, target=predicted_relative_relevance, log_target=False, reduction='batchmean')\n",
        "\n",
        "        # backward pass\n",
        "        total_loss = LabeledCELoss + args['lambda_ViewRegularization'] * ViewRegularizationLoss * current_warmup\n",
        "\n",
        "        total_loss.backward()\n",
        "\n",
        "\n",
        "\n",
        "        TotalLoss_this_epoch.append(total_loss.item())\n",
        "        LabeledCELoss_this_epoch.append(LabeledCELoss.item())\n",
        "        ViewRegularizationLoss_this_epoch.append(ViewRegularizationLoss.item())\n",
        "        scaled_ViewRegularizationLoss_this_epoch.append(args['lambda_ViewRegularization'] * ViewRegularizationLoss.item() * current_warmup)\n",
        "\n",
        "        # step\n",
        "        optimizer.step()\n",
        "\n",
        "        #update ema model\n",
        "        ema_model.update(model)\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    return TotalLoss_this_epoch, LabeledCELoss_this_epoch, ViewRegularizationLoss_this_epoch, scaled_ViewRegularizationLoss_this_epoch\n",
        "\n",
        "\n",
        "\n",
        "#regular eval_model\n",
        "def eval_model(args, data_loader, raw_model, ema_model, epoch):\n",
        "\n",
        "    raw_model.eval()\n",
        "    ema_model.eval()\n",
        "\n",
        "    data_loader = tqdm(data_loader, disable=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        total_targets = []\n",
        "        total_raw_outputs = []\n",
        "        total_ema_outputs = []\n",
        "\n",
        "\n",
        "        for batch_idx, (data, bag_label) in enumerate(data_loader):\n",
        "\n",
        "#             print('EVAL type(data): {}, data.size: {}, require grad: {}'.format(type(data), data.size(), data.requires_grad))\n",
        "#             print('EVAL type(bag_label): {}, bag_label: {}'.format(type(bag_label), bag_label))\n",
        "\n",
        "            data, bag_label = data.to(args['device']), bag_label.to(args['device'])\n",
        "\n",
        "            raw_outputs, raw_attention_weights = raw_model(data)\n",
        "            ema_outputs, ema_attention_weights = ema_model(data)\n",
        "#             print('target is {}, raw_outputs is: {}, ema_outputs is {}'.format(bag_label, raw_outputs, ema_outputs))\n",
        "\n",
        "            total_targets.append(bag_label.detach().cpu())\n",
        "            total_raw_outputs.append(raw_outputs.detach().cpu())\n",
        "            total_ema_outputs.append(ema_outputs.detach().cpu())\n",
        "\n",
        "\n",
        "        total_targets = np.concatenate(total_targets, axis=0)\n",
        "        total_raw_outputs = np.concatenate(total_raw_outputs, axis=0)\n",
        "        total_ema_outputs = np.concatenate(total_ema_outputs, axis=0)\n",
        "#         print('RegularEval total_targets: {}'.format(total_targets))\n",
        "#         print('RegularEval total_raw_outputs: {}'.format(total_raw_outputs))\n",
        "#         print('RegularEval total_ema_outputs: {}'.format(total_ema_outputs))\n",
        "\n",
        "        raw_Bacc = calculate_balanced_accuracy(total_raw_outputs, total_targets)\n",
        "        ema_Bacc = calculate_balanced_accuracy(total_ema_outputs, total_targets)\n",
        "\n",
        "#         print('raw Bacc this evaluation step: {}'.format(raw_Bacc), flush=True)\n",
        "#         print('ema Bacc this evaluation step: {}'.format(ema_Bacc), flush=True)\n",
        "\n",
        "\n",
        "    return raw_Bacc, ema_Bacc, total_targets, total_raw_outputs, total_ema_outputs\n",
        "\n",
        "def eval_model_test(args, data_loader, raw_model):\n",
        "    raw_model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        ground_truth_labels = []\n",
        "        pred_labels = []\n",
        "\n",
        "        for data, bag_label in data_loader:\n",
        "            data, bag_label = data.to(device), bag_label.to(device)\n",
        "\n",
        "            pred_logit, _ = raw_model(data)\n",
        "\n",
        "            pred_label = torch.softmax(pred_logit, dim=-1)\n",
        "            pred_label = torch.argmax(pred_label).item()\n",
        "\n",
        "            pred_labels.append(pred_label)\n",
        "            ground_truth_labels.append(bag_label.item())\n",
        "\n",
        "        bal_acc = balanced_accuracy_score(ground_truth_labels, pred_labels)\n",
        "\n",
        "    return bal_acc\n",
        "\n",
        "def calculate_balanced_accuracy(prediction, true_target, return_type = 'only balanced_accuracy'):\n",
        "\n",
        "    confusion_matrix = sklearn_cm(true_target, prediction.argmax(1))\n",
        "    n_class = confusion_matrix.shape[0]\n",
        "    print('Inside calculate_balanced_accuracy, {} classes passed in'.format(n_class), flush=True)\n",
        "\n",
        "    assert n_class==3\n",
        "\n",
        "    recalls = []\n",
        "    for i in range(n_class):\n",
        "        recall = confusion_matrix[i,i]/np.sum(confusion_matrix[i])\n",
        "        recalls.append(recall)\n",
        "        print('class{} recall: {}'.format(i, recall), flush=True)\n",
        "\n",
        "    balanced_accuracy = np.mean(np.array(recalls))\n",
        "\n",
        "\n",
        "    if return_type == 'all':\n",
        "#         return balanced_accuracy * 100, class0_recall * 100, class1_recall * 100, class2_recall * 100\n",
        "        return balanced_accuracy * 100, recalls\n",
        "\n",
        "    elif return_type == 'only balanced_accuracy':\n",
        "        return balanced_accuracy * 100\n",
        "    else:\n",
        "        raise NameError('Unsupported return_type in this calculate_balanced_accuracy fn')\n",
        "\n",
        "\n",
        " #shared helper fct across different algos\n",
        "def save_pickle(save_dir, save_file_name, data):\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    data_save_fullpath = os.path.join(save_dir, save_file_name)\n",
        "    with open(data_save_fullpath, 'wb') as handle:\n",
        "        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
      ],
      "metadata": {
        "id": "Ik5nwF5gV-6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training Runner\n",
        "The code block below contains the logic to train the model, one epoch at a time, with early stop, and the logic to write the results out. This method is called in subsequent training blocks after the training arguments are defined."
      ],
      "metadata": {
        "id": "iee2gbDtxiJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def train_samil(args):\n",
        "    best_val_ema_Bacc = 0\n",
        "    best_test_ema_Bacc_at_val = 0\n",
        "    best_train_ema_Bacc_at_val = 0\n",
        "\n",
        "    best_val_raw_Bacc = 0\n",
        "    best_test_raw_Bacc_at_val = 0\n",
        "    best_train_raw_Bacc_at_val = 0\n",
        "\n",
        "\n",
        "    current_count=0\n",
        "\n",
        "    if os.path.isfile(args.get('resume_checkpoint_fullpath')):\n",
        "\n",
        "        print('Resuming from checkpoint: {}'.format(args.get('resume_checkpoint_fullpath')))\n",
        "\n",
        "        checkpoint = torch.load(args['resume_checkpoint_fullpath'])\n",
        "        args['start_epoch'] = checkpoint['epoch']\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "        ema_model.ema.load_state_dict(checkpoint['ema_state_dict'])\n",
        "        current_count = checkpoint['current_count']\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        scheduler.load_state_dict(checkpoint['scheduler'])\n",
        "\n",
        "        best_val_ema_Bacc = checkpoint['val_progression_view']['best_val_ema_Bacc']\n",
        "        best_test_ema_Bacc_at_val = checkpoint['val_progression_view']['best_test_ema_Bacc_at_val']\n",
        "        best_train_ema_Bacc_at_val = checkpoint['val_progression_view']['best_train_ema_Bacc_at_val']\n",
        "\n",
        "        best_val_raw_Bacc = checkpoint['val_progression_view']['best_val_raw_Bacc']\n",
        "        best_test_raw_Bacc_at_val = checkpoint['val_progression_view']['best_test_raw_Bacc_at_val']\n",
        "        best_train_raw_Bacc_at_val = checkpoint['val_progression_view']['best_train_raw_Bacc_at_val']\n",
        "\n",
        "\n",
        "    else:\n",
        "        print('!!!!Does not have checkpoint yet!!!!')\n",
        "\n",
        "\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(f\"  Task = {args['dataset_name']}\")\n",
        "    logger.info(f\"  Num Epochs = {args['train_epoch']}\")\n",
        "    logger.info(f\"  Total optimization steps = {args['train_epoch'] * len(train_dataset)}\")\n",
        "\n",
        "    train_loss_dict = dict()\n",
        "    train_loss_dict['Totalloss'] = []\n",
        "    train_loss_dict['LabeledCEloss'] = []\n",
        "    train_loss_dict['ViewRegularizationLoss'] = []\n",
        "\n",
        "    early_stopping = EarlyStopping(patience=args['patience'], initial_count=current_count)\n",
        "    early_stopping_warmup = args['early_stopping_warmup']\n",
        "\n",
        "    for epoch in tqdm(range(args['start_epoch'], args['train_epoch'])):\n",
        "        val_predictions_save_dict = dict()\n",
        "        test_predictions_save_dict = dict()\n",
        "        train_predictions_save_dict = dict()\n",
        "\n",
        "        TotalLoss_list, LabeledCEloss_list, ViewRegularizationLoss_list, scaled_ViewRegularizationLoss_list = train_one_epoch(args, weights, train_loader, model, ema_model, view_model, optimizer, scheduler, epoch)\n",
        "        train_loss_dict['Totalloss'].extend(TotalLoss_list)\n",
        "        train_loss_dict['LabeledCEloss'].extend(LabeledCEloss_list)\n",
        "        train_loss_dict['ViewRegularizationLoss'].extend(ViewRegularizationLoss_list)\n",
        "\n",
        "        if epoch % args['eval_every_Xepoch'] == 0:\n",
        "            val_raw_Bacc, val_ema_Bacc, val_true_labels, val_raw_predictions, val_ema_predictions = eval_model(args, val_loader, model, ema_model.ema, epoch)\n",
        "            val_predictions_save_dict['raw_Bacc'] = val_raw_Bacc\n",
        "            val_predictions_save_dict['ema_Bacc'] = val_ema_Bacc\n",
        "            val_predictions_save_dict['true_labels'] = val_true_labels\n",
        "            val_predictions_save_dict['raw_predictions'] = val_raw_predictions\n",
        "            val_predictions_save_dict['ema_predictions'] = val_ema_predictions\n",
        "\n",
        "            test_raw_Bacc, test_ema_Bacc, test_true_labels, test_raw_predictions, test_ema_predictions = eval_model(args, test_loader, model, ema_model.ema, epoch)\n",
        "\n",
        "            test_predictions_save_dict['raw_Bacc'] = test_raw_Bacc\n",
        "            test_predictions_save_dict['ema_Bacc'] = test_ema_Bacc\n",
        "            test_predictions_save_dict['true_labels'] = test_true_labels\n",
        "            test_predictions_save_dict['raw_predictions'] = test_raw_predictions\n",
        "            test_predictions_save_dict['ema_predictions'] = test_ema_predictions\n",
        "\n",
        "            train_raw_Bacc, train_ema_Bacc, train_true_labels, train_raw_predictions, train_ema_predictions = eval_model(args, trainmemory_loader, model, ema_model.ema, epoch)\n",
        "\n",
        "            train_predictions_save_dict['raw_Bacc'] = train_raw_Bacc\n",
        "            train_predictions_save_dict['ema_Bacc'] = train_ema_Bacc\n",
        "            train_predictions_save_dict['true_labels'] = train_true_labels\n",
        "            train_predictions_save_dict['raw_predictions'] = train_raw_predictions\n",
        "            train_predictions_save_dict['ema_predictions'] = train_ema_predictions\n",
        "\n",
        "            if val_raw_Bacc > best_val_raw_Bacc:\n",
        "\n",
        "                best_val_raw_Bacc = val_raw_Bacc\n",
        "                best_test_raw_Bacc_at_val = test_raw_Bacc\n",
        "                best_train_raw_Bacc_at_val = train_raw_Bacc\n",
        "\n",
        "                save_pickle(os.path.join(args['experiment_dir'], 'val_progression_view', 'best_predictions_at_raw_val'), 'val_predictions.pkl', val_predictions_save_dict)\n",
        "\n",
        "                save_pickle(os.path.join(args['experiment_dir'], 'val_progression_view', 'best_predictions_at_raw_val'), 'test_predictions.pkl', test_predictions_save_dict)\n",
        "\n",
        "\n",
        "                save_pickle(os.path.join(args['experiment_dir'], 'val_progression_view', 'best_predictions_at_raw_val'), 'train_predictions.pkl', train_predictions_save_dict)\n",
        "\n",
        "                save_checkpoint(\n",
        "                {\n",
        "                'epoch': epoch+1,\n",
        "                'state_dict': model.state_dict(),\n",
        "                'ema_state_dict': ema_model.ema.state_dict(),\n",
        "                'current_count':current_count,\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "                'scheduler': scheduler.state_dict(),\n",
        "\n",
        "                'val_progression_view':\n",
        "                    {'epoch': epoch+1,\n",
        "                    'best_val_ema_Bacc': best_val_ema_Bacc,\n",
        "                    'best_val_raw_Bacc': best_val_raw_Bacc,\n",
        "                    'best_test_ema_Bacc_at_val': best_test_ema_Bacc_at_val,\n",
        "                    'best_test_raw_Bacc_at_val': best_test_raw_Bacc_at_val,\n",
        "                    'best_train_ema_Bacc_at_val': best_train_ema_Bacc_at_val,\n",
        "                    'best_train_raw_Bacc_at_val': best_train_raw_Bacc_at_val,\n",
        "                      },\n",
        "\n",
        "                }, args['experiment_dir'], filename='val_progression_view/best_predictions_at_raw_val/best_model.pth.tar')\n",
        "\n",
        "\n",
        "            if val_ema_Bacc > best_val_ema_Bacc:\n",
        "\n",
        "                best_val_ema_Bacc = val_ema_Bacc\n",
        "                best_test_ema_Bacc_at_val = test_ema_Bacc\n",
        "                best_train_ema_Bacc_at_val = train_ema_Bacc\n",
        "\n",
        "                save_pickle(os.path.join(args['experiment_dir'], 'val_progression_view', 'best_predictions_at_ema_val'), 'val_predictions.pkl', val_predictions_save_dict)\n",
        "\n",
        "                save_pickle(os.path.join(args['experiment_dir'], 'val_progression_view', 'best_predictions_at_ema_val'), 'test_predictions.pkl', test_predictions_save_dict)\n",
        "\n",
        "                save_pickle(os.path.join(args['experiment_dir'], 'val_progression_view', 'best_predictions_at_ema_val'), 'train_predictions.pkl', train_predictions_save_dict)\n",
        "\n",
        "                save_checkpoint(\n",
        "                {\n",
        "                'epoch': epoch+1,\n",
        "                'state_dict': model.state_dict(),\n",
        "                'ema_state_dict': ema_model.ema.state_dict(),\n",
        "                'current_count':current_count,\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "                'scheduler': scheduler.state_dict(),\n",
        "\n",
        "                'val_progression_view':\n",
        "                    {'epoch': epoch+1,\n",
        "                    #regular val\n",
        "                    'best_val_ema_Bacc': best_val_ema_Bacc,\n",
        "                    'best_val_raw_Bacc': best_val_raw_Bacc,\n",
        "                    'best_test_ema_Bacc_at_val': best_test_ema_Bacc_at_val,\n",
        "                    'best_test_raw_Bacc_at_val': best_test_raw_Bacc_at_val,\n",
        "                    'best_train_ema_Bacc_at_val': best_train_ema_Bacc_at_val,\n",
        "                    'best_train_raw_Bacc_at_val': best_train_raw_Bacc_at_val,\n",
        "                      },\n",
        "\n",
        "                }, args['experiment_dir'], filename='val_progression_view/best_predictions_at_ema_val/best_model.pth.tar')\n",
        "\n",
        "\n",
        "\n",
        "            logger.info('val progression view:')\n",
        "            logger.info('At RAW Best val, validation/test/train %.2f %.2f %.2f' % (best_val_raw_Bacc, best_test_raw_Bacc_at_val, best_train_raw_Bacc_at_val))\n",
        "            logger.info('At EMA Best val, validation/test/train %.2f %.2f %.2f' % (best_val_ema_Bacc, best_test_ema_Bacc_at_val, best_train_ema_Bacc_at_val))\n",
        "\n",
        "            args['writer'].add_scalar('train/1.train_raw_Bacc', train_raw_Bacc, epoch)\n",
        "            args['writer'].add_scalar('train/1.train_ema_Bacc', train_ema_Bacc, epoch)\n",
        "            args['writer'].add_scalar('train/1.LabeledCEloss', np.mean(LabeledCEloss_list), epoch)\n",
        "\n",
        "            args['writer'].add_scalar('val/1.val_raw_Bacc', val_raw_Bacc, epoch)\n",
        "            args['writer'].add_scalar('val/2.val_ema_Bacc', val_ema_Bacc, epoch)\n",
        "\n",
        "\n",
        "            args['writer'].add_scalar('test/1.test_raw_Bacc', test_raw_Bacc, epoch)\n",
        "            args['writer'].add_scalar('test/2.test_ema_Bacc', test_ema_Bacc, epoch)\n",
        "\n",
        "            brief_summary['val_progression_view']['best_val_ema_Bacc'] = best_val_ema_Bacc\n",
        "            brief_summary['val_progression_view']['best_val_raw_Bacc'] = best_val_raw_Bacc\n",
        "            brief_summary['val_progression_view']['best_test_ema_Bacc_at_val'] = best_test_ema_Bacc_at_val\n",
        "            brief_summary['val_progression_view']['best_test_raw_Bacc_at_val'] = best_test_raw_Bacc_at_val\n",
        "            brief_summary['val_progression_view']['best_train_ema_Bacc_at_val'] = best_train_ema_Bacc_at_val\n",
        "            brief_summary['val_progression_view']['best_train_raw_Bacc_at_val'] = best_train_raw_Bacc_at_val\n",
        "\n",
        "\n",
        "            with open(os.path.join(args['experiment_dir'], \"brief_summary.json\"), \"w\") as f:\n",
        "                json.dump(brief_summary, f)\n",
        "\n",
        "            if epoch > early_stopping_warmup:\n",
        "                current_count = early_stopping(val_ema_Bacc)\n",
        "\n",
        "            save_checkpoint(\n",
        "                {\n",
        "                'epoch': epoch+1,\n",
        "                'state_dict': model.state_dict(),\n",
        "                'ema_state_dict': ema_model.ema.state_dict(),\n",
        "                'current_count':current_count,\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "                'scheduler': scheduler.state_dict(),\n",
        "\n",
        "                'val_progression_view':\n",
        "                    {'epoch': epoch+1,\n",
        "                    #regular val\n",
        "                    'best_val_ema_Bacc': best_val_ema_Bacc,\n",
        "                    'best_val_raw_Bacc': best_val_raw_Bacc,\n",
        "                    'best_test_ema_Bacc_at_val': best_test_ema_Bacc_at_val,\n",
        "                    'best_test_raw_Bacc_at_val': best_test_raw_Bacc_at_val,\n",
        "                    'best_train_ema_Bacc_at_val': best_train_ema_Bacc_at_val,\n",
        "                    'best_train_raw_Bacc_at_val': best_train_raw_Bacc_at_val,\n",
        "                      },\n",
        "\n",
        "\n",
        "                }, args['experiment_dir'], filename='last_checkpoint.pth.tar')\n",
        "\n",
        "\n",
        "            if early_stopping.early_stop:\n",
        "                break\n",
        "\n",
        "    brief_summary['val_progression_view']['best_val_ema_Bacc'] = best_val_ema_Bacc\n",
        "    brief_summary['val_progression_view']['best_val_raw_Bacc'] = best_val_raw_Bacc\n",
        "    brief_summary['val_progression_view']['best_test_ema_Bacc_at_val'] = best_test_ema_Bacc_at_val\n",
        "    brief_summary['val_progression_view']['best_test_raw_Bacc_at_val'] = best_test_raw_Bacc_at_val\n",
        "    brief_summary['val_progression_view']['best_train_ema_Bacc_at_val'] = best_train_ema_Bacc_at_val\n",
        "    brief_summary['val_progression_view']['best_train_raw_Bacc_at_val'] = best_train_raw_Bacc_at_val\n",
        "\n",
        "\n",
        "\n",
        "    args['writer'].close()\n",
        "\n",
        "    with open(os.path.join(args['experiment_dir'], \"brief_summary.json\"), \"w\") as f:\n",
        "        json.dump(brief_summary, f)"
      ],
      "metadata": {
        "id": "CIP1YPVGXIro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure SAMIL Arguments for Training\n",
        "\n",
        "The (3) code blocks below configure the arguments for training the SAMIL model with no pretraining, with image-level pretraining, and study-level pretraining per the specified hyperparameters in the paper for Split 1 and the Github repo here: [Hyperparameters](https://github.com/tufts-ml/SAMIL/blob/main/Hyperparameters/Hyperparameters.txt)\n",
        "\n",
        "| SAMIL (with study-level SSL) | split1 | split2 | split3 |\n",
        "| ---------------------------- | ------ | ------ | ------ |\n",
        "| Learning rate                | 0.0008 | 0.0005 | 0.0005 |\n",
        "| Weight decay                 | 0.0001 | 0.0001 | 0.001  |\n",
        "| Temperature T                | 0.1    | 0.05   | 0.1    |\n",
        "| λ<sub>sA</sub>               | 15.0   | 20.0   | 20.0   |\n",
        "| Learning rate schedule       | cosine | cosine | cosine |\n",
        "\n",
        "Table C.1: Hyperparameter settings for SAMIL across different data splits.\n"
      ],
      "metadata": {
        "id": "QR2XLsj8XDru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Computational Requirements\n",
        "\n",
        "It is highly recommended to train the model variants on A100 GPUs as suggested in the paper. Training in a stable environment (not in Google Colab) is preferable due to the long training times necessary for each variant.\n",
        "\n",
        "For the SAMIL models on an A100 GPU in Colab, each epoch was taking an average of 22 seconds. This represents an upper bound of roughly 13 hours without Early Stopping."
      ],
      "metadata": {
        "id": "kQAOpOARX95G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SAMIL with No Pretraining\n",
        "\n",
        "The arguments below setup the runner to train SAMIL with no pretraining. This is achieved by specifying `NoPretrain` in the `Pretrained` argument.\n",
        "\n",
        "The number of epochs specified in the paper is 2,000, however the arguments below are set to 1 to enable the testing of this training run."
      ],
      "metadata": {
        "id": "T9_X1M2WWil2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RUNS_DIR = '/content/runs/'\n",
        "\n",
        "args = {\n",
        "    'training_seed': 0,\n",
        "    'Pretrained': 'NoPretrain',\n",
        "    'data_seed': 0,\n",
        "    'checkpoint_dir': MODEL_CHECKPOINTS,\n",
        "    'MIL_checkpoint_path': '',\n",
        "    'use_class_weights': 'True',\n",
        "    'ViewRegularization_warmup_schedule_type': 'Linear',\n",
        "    'optimizer_type': 'SGD',\n",
        "    'lr_schedule_type': 'CosineLR',\n",
        "    'lr_cycle_epochs': 1,\n",
        "    'lr': 0.0008, # learning rate\n",
        "    'wd': 0.0001, # weight decay\n",
        "    'T': 0.1, # tempertature\n",
        "    'lambda_ViewRegularization': 15.0, # λsA\n",
        "    'train_dir': RUNS_DIR + 'SAMIL',\n",
        "    'resume': 'last_checkpoint.pth.tar',\n",
        "    'dataset_name': 'echo',\n",
        "    'train_epoch': 2, # number of epochs, 2000 defined in the paper. CHANGE ME!\n",
        "    'development_size': 'DEV479',\n",
        "    'lr_warmup_epochs': 0,\n",
        "    'ema_decay': 0.999,\n",
        "    'device': device,\n",
        "    'start_epoch': 0,\n",
        "    'patience': 200,\n",
        "    'early_stopping_warmup': 200,\n",
        "    'ViewRegularization_warmup_pos': 0.4,\n",
        "    'eval_every_Xepoch': 1\n",
        "}\n",
        "\n",
        "args, brief_summary = setup_samil_train(args)\n",
        "\n",
        "weights = args['class_weights']\n",
        "weights = [float(i) for i in weights.split(',')]\n",
        "weights = torch.Tensor(weights)\n",
        "weights = weights.to(device)\n",
        "\n",
        "#load the view model, the output is unnormalized logits, need to use softmax on the output\n",
        "view_model = create_view_model(args)\n",
        "view_model.to(device)\n",
        "\n",
        "model = create_model(args)\n",
        "model.to(device)\n",
        "\n",
        "no_decay = ['bias', 'bn']\n",
        "grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(\n",
        "        nd in n for nd in no_decay)], 'weight_decay': args['wd']},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(\n",
        "        nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "\n",
        "if args['optimizer_type'] == 'SGD':\n",
        "    optimizer = optim.SGD(grouped_parameters, lr=args['lr'],\n",
        "                          momentum=0.9, nesterov=True)\n",
        "\n",
        "elif args['optimizer_type'] == 'Adam':\n",
        "    optimizer = optim.Adam(grouped_parameters, lr=args['lr'])\n",
        "\n",
        "elif args['optimizer_type'] == 'AdamW':\n",
        "    optimizer = optim.AdamW(grouped_parameters, lr=args['lr'])\n",
        "\n",
        "else:\n",
        "    raise NameError('Not supported optimizer setting')\n",
        "\n",
        "#lr_schedule_type choice\n",
        "if args['lr_schedule_type'] == 'CosineLR':\n",
        "    scheduler = get_cosine_schedule_with_warmup(optimizer, args['lr_warmup_epochs'], args['lr_cycle_epochs'])\n",
        "\n",
        "elif args['lr_schedule_type'] == 'FixedLR':\n",
        "    scheduler = get_fixed_lr(optimizer, args['lr_warmup_epochs'], args['lr_cycle_epochs'])\n",
        "\n",
        "else:\n",
        "    raise NameError('Not supported lr scheduler setting')\n",
        "\n",
        "\n",
        "#instantiate the ema_model object\n",
        "ema_model = ModelEMA(args, model, args['ema_decay'])\n",
        "\n",
        "\n",
        "# !!! Start training\n",
        "train_samil(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fQSIs58PUZc",
        "outputId": "1ba261df-4c14-4b8f-eb9c-5144e58aa61d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "setting training seed0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Model: WideResNet 28x2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!!!!!!!Using pre-calculated class weights!!!!!!!!\n",
            "args.resume_checkpoint_fullpath: /content/runs/SAMIL/NoPretrain/last_checkpoint.pth.tar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Total params for View Model: 5.93M\n",
            "INFO:__main__:Total params: 2.31M\n",
            "INFO:__main__:***** Running training *****\n",
            "INFO:__main__:  Task = echo\n",
            "INFO:__main__:  Num Epochs = 2\n",
            "INFO:__main__:  Total optimization steps = 720\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "self.param_keys: ['feature_extractor_part1.0.weight', 'feature_extractor_part1.0.bias', 'feature_extractor_part1.3.weight', 'feature_extractor_part1.3.bias', 'feature_extractor_part1.6.weight', 'feature_extractor_part1.6.bias', 'feature_extractor_part1.9.weight', 'feature_extractor_part1.9.bias', 'feature_extractor_part2.0.weight', 'feature_extractor_part2.0.bias', 'feature_extractor_part3.0.weight', 'feature_extractor_part3.0.bias', 'feature_extractor_part3.2.weight', 'feature_extractor_part3.2.bias', 'attention_V.0.weight', 'attention_V.0.bias', 'attention_V.2.weight', 'attention_V.2.bias', 'attention_U.0.weight', 'attention_U.0.bias', 'attention_U.2.weight', 'attention_U.2.bias', 'classifier.0.weight', 'classifier.0.bias']\n",
            "self.buffer_keys: []\n",
            "Resuming from checkpoint: /content/runs/SAMIL/NoPretrain/last_checkpoint.pth.tar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/1 [00:00<?, ?it/s]\n",
            "  0%|          | 0/360 [00:40<?, ?it/s]\n",
            "\n",
            "  0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/119 [00:00<01:25,  1.38it/s]\u001b[A\n",
            "  2%|▏         | 2/119 [00:00<00:51,  2.25it/s]\u001b[A\n",
            "  4%|▍         | 5/119 [00:01<00:18,  6.33it/s]\u001b[A\n",
            "  7%|▋         | 8/119 [00:01<00:11,  9.57it/s]\u001b[A\n",
            "  9%|▉         | 11/119 [00:01<00:08, 12.85it/s]\u001b[A\n",
            " 13%|█▎        | 15/119 [00:01<00:05, 17.74it/s]\u001b[A\n",
            " 15%|█▌        | 18/119 [00:01<00:05, 18.77it/s]\u001b[A\n",
            " 18%|█▊        | 22/119 [00:01<00:04, 22.46it/s]\u001b[A\n",
            " 21%|██        | 25/119 [00:01<00:04, 23.36it/s]\u001b[A\n",
            " 24%|██▍       | 29/119 [00:01<00:03, 27.31it/s]\u001b[A\n",
            " 28%|██▊       | 33/119 [00:02<00:03, 27.76it/s]\u001b[A\n",
            " 30%|███       | 36/119 [00:02<00:03, 27.33it/s]\u001b[A\n",
            " 33%|███▎      | 39/119 [00:02<00:02, 27.24it/s]\u001b[A\n",
            " 35%|███▌      | 42/119 [00:02<00:02, 27.37it/s]\u001b[A\n",
            " 39%|███▊      | 46/119 [00:02<00:02, 29.59it/s]\u001b[A\n",
            " 42%|████▏     | 50/119 [00:02<00:02, 28.65it/s]\u001b[A\n",
            " 45%|████▍     | 53/119 [00:02<00:02, 27.54it/s]\u001b[A\n",
            " 48%|████▊     | 57/119 [00:02<00:02, 28.82it/s]\u001b[A\n",
            " 52%|█████▏    | 62/119 [00:03<00:01, 31.31it/s]\u001b[A\n",
            " 55%|█████▌    | 66/119 [00:03<00:01, 32.75it/s]\u001b[A\n",
            " 59%|█████▉    | 70/119 [00:03<00:01, 31.20it/s]\u001b[A\n",
            " 62%|██████▏   | 74/119 [00:03<00:01, 31.49it/s]\u001b[A\n",
            " 66%|██████▌   | 78/119 [00:03<00:01, 29.81it/s]\u001b[A\n",
            " 69%|██████▉   | 82/119 [00:03<00:01, 30.08it/s]\u001b[A\n",
            " 72%|███████▏  | 86/119 [00:03<00:01, 29.13it/s]\u001b[A\n",
            " 76%|███████▌  | 90/119 [00:04<00:00, 31.27it/s]\u001b[A\n",
            " 79%|███████▉  | 94/119 [00:04<00:00, 30.62it/s]\u001b[A\n",
            " 82%|████████▏ | 98/119 [00:04<00:00, 29.34it/s]\u001b[A\n",
            " 85%|████████▍ | 101/119 [00:04<00:00, 28.77it/s]\u001b[A\n",
            " 87%|████████▋ | 104/119 [00:04<00:00, 28.74it/s]\u001b[A\n",
            " 93%|█████████▎| 111/119 [00:04<00:00, 38.07it/s]\u001b[A\n",
            "100%|██████████| 119/119 [00:05<00:00, 23.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inside calculate_balanced_accuracy, 3 classes passed in\n",
            "class0 recall: 0.0\n",
            "class1 recall: 0.0\n",
            "class2 recall: 1.0\n",
            "Inside calculate_balanced_accuracy, 3 classes passed in\n",
            "class0 recall: 0.0\n",
            "class1 recall: 0.0\n",
            "class2 recall: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0/120 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/120 [00:00<01:45,  1.13it/s]\u001b[A\n",
            "  2%|▎         | 3/120 [00:01<00:33,  3.51it/s]\u001b[A\n",
            "  5%|▌         | 6/120 [00:01<00:15,  7.33it/s]\u001b[A\n",
            "  8%|▊         | 10/120 [00:01<00:08, 12.71it/s]\u001b[A\n",
            " 12%|█▏        | 14/120 [00:01<00:06, 17.02it/s]\u001b[A\n",
            " 15%|█▌        | 18/120 [00:01<00:04, 21.34it/s]\u001b[A\n",
            " 18%|█▊        | 22/120 [00:01<00:03, 24.73it/s]\u001b[A\n",
            " 22%|██▏       | 26/120 [00:01<00:03, 24.73it/s]\u001b[A\n",
            " 24%|██▍       | 29/120 [00:01<00:03, 24.33it/s]\u001b[A\n",
            " 28%|██▊       | 33/120 [00:02<00:03, 26.39it/s]\u001b[A\n",
            " 30%|███       | 36/120 [00:02<00:03, 26.63it/s]\u001b[A\n",
            " 33%|███▎      | 40/120 [00:02<00:02, 28.87it/s]\u001b[A\n",
            " 38%|███▊      | 45/120 [00:02<00:02, 33.08it/s]\u001b[A\n",
            " 41%|████      | 49/120 [00:02<00:02, 29.03it/s]\u001b[A\n",
            " 44%|████▍     | 53/120 [00:02<00:02, 29.47it/s]\u001b[A\n",
            " 48%|████▊     | 57/120 [00:02<00:02, 27.28it/s]\u001b[A\n",
            " 50%|█████     | 60/120 [00:03<00:02, 24.92it/s]\u001b[A\n",
            " 52%|█████▎    | 63/120 [00:03<00:02, 25.07it/s]\u001b[A\n",
            " 55%|█████▌    | 66/120 [00:03<00:02, 22.09it/s]\u001b[A\n",
            " 57%|█████▊    | 69/120 [00:03<00:02, 20.47it/s]\u001b[A\n",
            " 60%|██████    | 72/120 [00:03<00:02, 20.70it/s]\u001b[A\n",
            " 62%|██████▎   | 75/120 [00:03<00:02, 18.79it/s]\u001b[A\n",
            " 64%|██████▍   | 77/120 [00:03<00:02, 17.73it/s]\u001b[A\n",
            " 66%|██████▌   | 79/120 [00:04<00:02, 17.01it/s]\u001b[A\n",
            " 68%|██████▊   | 82/120 [00:04<00:02, 18.28it/s]\u001b[A\n",
            " 70%|███████   | 84/120 [00:04<00:02, 16.24it/s]\u001b[A\n",
            " 72%|███████▏  | 86/120 [00:04<00:01, 17.01it/s]\u001b[A\n",
            " 73%|███████▎  | 88/120 [00:04<00:01, 17.41it/s]\u001b[A\n",
            " 75%|███████▌  | 90/120 [00:04<00:01, 17.06it/s]\u001b[A\n",
            " 78%|███████▊  | 93/120 [00:04<00:01, 20.06it/s]\u001b[A\n",
            " 80%|████████  | 96/120 [00:05<00:01, 18.23it/s]\u001b[A\n",
            " 82%|████████▏ | 98/120 [00:05<00:01, 17.51it/s]\u001b[A\n",
            " 84%|████████▍ | 101/120 [00:05<00:01, 18.20it/s]\u001b[A\n",
            " 87%|████████▋ | 104/120 [00:05<00:00, 19.24it/s]\u001b[A\n",
            " 91%|█████████ | 109/120 [00:05<00:00, 25.66it/s]\u001b[A\n",
            "100%|██████████| 120/120 [00:06<00:00, 18.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inside calculate_balanced_accuracy, 3 classes passed in\n",
            "class0 recall: 0.0\n",
            "class1 recall: 0.0\n",
            "class2 recall: 1.0\n",
            "Inside calculate_balanced_accuracy, 3 classes passed in\n",
            "class0 recall: 0.0\n",
            "class1 recall: 0.0\n",
            "class2 recall: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0/360 [00:00<?, ?it/s]\u001b[A\n",
            "  0%|          | 1/360 [00:00<04:59,  1.20it/s]\u001b[A\n",
            "  1%|          | 2/360 [00:01<03:04,  1.94it/s]\u001b[A\n",
            "  1%|          | 3/360 [00:01<02:00,  2.96it/s]\u001b[A\n",
            "  2%|▏         | 6/360 [00:01<00:48,  7.31it/s]\u001b[A\n",
            "  3%|▎         | 10/360 [00:01<00:27, 12.88it/s]\u001b[A\n",
            "  4%|▎         | 13/360 [00:01<00:23, 14.67it/s]\u001b[A\n",
            "  5%|▍         | 17/360 [00:01<00:17, 19.28it/s]\u001b[A\n",
            "  6%|▌         | 20/360 [00:01<00:16, 21.23it/s]\u001b[A\n",
            "  7%|▋         | 24/360 [00:01<00:13, 24.84it/s]\u001b[A\n",
            "  8%|▊         | 30/360 [00:02<00:10, 30.71it/s]\u001b[A\n",
            "  9%|▉         | 34/360 [00:02<00:12, 27.01it/s]\u001b[A\n",
            " 10%|█         | 37/360 [00:02<00:11, 27.42it/s]\u001b[A\n",
            " 11%|█▏        | 41/360 [00:02<00:10, 29.13it/s]\u001b[A\n",
            " 12%|█▎        | 45/360 [00:02<00:10, 30.50it/s]\u001b[A\n",
            " 14%|█▎        | 49/360 [00:02<00:09, 32.57it/s]\u001b[A\n",
            " 15%|█▍        | 53/360 [00:02<00:09, 33.08it/s]\u001b[A\n",
            " 16%|█▌        | 57/360 [00:03<00:09, 31.18it/s]\u001b[A\n",
            " 17%|█▋        | 61/360 [00:03<00:09, 29.94it/s]\u001b[A\n",
            " 18%|█▊        | 65/360 [00:03<00:09, 30.54it/s]\u001b[A\n",
            " 19%|█▉        | 69/360 [00:03<00:09, 31.45it/s]\u001b[A\n",
            " 20%|██        | 73/360 [00:03<00:10, 27.03it/s]\u001b[A\n",
            " 21%|██        | 76/360 [00:03<00:12, 23.29it/s]\u001b[A\n",
            " 22%|██▏       | 79/360 [00:03<00:12, 22.10it/s]\u001b[A\n",
            " 23%|██▎       | 82/360 [00:04<00:12, 22.77it/s]\u001b[A\n",
            " 24%|██▎       | 85/360 [00:04<00:11, 24.38it/s]\u001b[A\n",
            " 24%|██▍       | 88/360 [00:04<00:11, 23.39it/s]\u001b[A\n",
            " 25%|██▌       | 91/360 [00:04<00:11, 24.33it/s]\u001b[A\n",
            " 26%|██▌       | 94/360 [00:04<00:11, 23.07it/s]\u001b[A\n",
            " 27%|██▋       | 97/360 [00:04<00:10, 24.12it/s]\u001b[A\n",
            " 28%|██▊       | 100/360 [00:04<00:11, 22.23it/s]\u001b[A\n",
            " 29%|██▊       | 103/360 [00:04<00:10, 23.96it/s]\u001b[A\n",
            " 30%|██▉       | 107/360 [00:05<00:09, 26.40it/s]\u001b[A\n",
            " 31%|███       | 110/360 [00:05<00:09, 26.54it/s]\u001b[A\n",
            " 31%|███▏      | 113/360 [00:05<00:09, 27.25it/s]\u001b[A\n",
            " 33%|███▎      | 118/360 [00:05<00:07, 31.32it/s]\u001b[A\n",
            " 34%|███▍      | 123/360 [00:05<00:06, 34.39it/s]\u001b[A\n",
            " 35%|███▌      | 127/360 [00:05<00:06, 33.34it/s]\u001b[A\n",
            " 36%|███▋      | 131/360 [00:05<00:07, 31.09it/s]\u001b[A\n",
            " 38%|███▊      | 135/360 [00:05<00:07, 30.36it/s]\u001b[A\n",
            " 39%|███▊      | 139/360 [00:06<00:06, 32.40it/s]\u001b[A\n",
            " 40%|███▉      | 143/360 [00:06<00:07, 30.33it/s]\u001b[A\n",
            " 41%|████      | 147/360 [00:06<00:07, 30.20it/s]\u001b[A\n",
            " 42%|████▏     | 151/360 [00:06<00:06, 30.70it/s]\u001b[A\n",
            " 43%|████▎     | 155/360 [00:06<00:06, 30.03it/s]\u001b[A\n",
            " 44%|████▍     | 159/360 [00:06<00:06, 30.93it/s]\u001b[A\n",
            " 45%|████▌     | 163/360 [00:06<00:06, 29.27it/s]\u001b[A\n",
            " 46%|████▌     | 166/360 [00:07<00:07, 26.78it/s]\u001b[A\n",
            " 47%|████▋     | 170/360 [00:07<00:06, 29.62it/s]\u001b[A\n",
            " 48%|████▊     | 174/360 [00:07<00:06, 28.70it/s]\u001b[A\n",
            " 49%|████▉     | 177/360 [00:07<00:06, 28.55it/s]\u001b[A\n",
            " 50%|█████     | 181/360 [00:07<00:06, 29.43it/s]\u001b[A\n",
            " 52%|█████▏    | 186/360 [00:07<00:05, 32.51it/s]\u001b[A\n",
            " 53%|█████▎    | 190/360 [00:07<00:04, 34.02it/s]\u001b[A\n",
            " 54%|█████▍    | 195/360 [00:07<00:04, 35.95it/s]\u001b[A\n",
            " 55%|█████▌    | 199/360 [00:07<00:04, 33.15it/s]\u001b[A\n",
            " 56%|█████▋    | 203/360 [00:08<00:04, 32.42it/s]\u001b[A\n",
            " 58%|█████▊    | 208/360 [00:08<00:04, 34.58it/s]\u001b[A\n",
            " 59%|█████▉    | 213/360 [00:08<00:04, 36.41it/s]\u001b[A\n",
            " 60%|██████    | 217/360 [00:08<00:04, 29.94it/s]\u001b[A\n",
            " 61%|██████▏   | 221/360 [00:08<00:04, 31.04it/s]\u001b[A\n",
            " 62%|██████▎   | 225/360 [00:08<00:04, 30.56it/s]\u001b[A\n",
            " 64%|██████▎   | 229/360 [00:08<00:04, 27.85it/s]\u001b[A\n",
            " 64%|██████▍   | 232/360 [00:09<00:04, 28.21it/s]\u001b[A\n",
            " 66%|██████▌   | 236/360 [00:09<00:04, 29.73it/s]\u001b[A\n",
            " 67%|██████▋   | 240/360 [00:09<00:04, 29.91it/s]\u001b[A\n",
            " 68%|██████▊   | 244/360 [00:09<00:03, 30.78it/s]\u001b[A\n",
            " 69%|██████▉   | 248/360 [00:09<00:03, 29.77it/s]\u001b[A\n",
            " 70%|███████   | 252/360 [00:09<00:03, 29.99it/s]\u001b[A\n",
            " 71%|███████   | 256/360 [00:09<00:03, 30.01it/s]\u001b[A\n",
            " 72%|███████▏  | 260/360 [00:10<00:03, 29.86it/s]\u001b[A\n",
            " 74%|███████▎  | 265/360 [00:10<00:03, 30.93it/s]\u001b[A\n",
            " 75%|███████▍  | 269/360 [00:10<00:03, 27.74it/s]\u001b[A\n",
            " 76%|███████▌  | 273/360 [00:10<00:03, 28.77it/s]\u001b[A\n",
            " 77%|███████▋  | 277/360 [00:10<00:02, 29.77it/s]\u001b[A\n",
            " 78%|███████▊  | 281/360 [00:10<00:02, 29.44it/s]\u001b[A\n",
            " 79%|███████▉  | 284/360 [00:10<00:03, 25.08it/s]\u001b[A\n",
            " 80%|███████▉  | 287/360 [00:11<00:03, 23.10it/s]\u001b[A\n",
            " 81%|████████  | 290/360 [00:11<00:03, 21.63it/s]\u001b[A\n",
            " 82%|████████▏ | 294/360 [00:11<00:02, 24.36it/s]\u001b[A\n",
            " 82%|████████▎ | 297/360 [00:11<00:03, 20.75it/s]\u001b[A\n",
            " 83%|████████▎ | 300/360 [00:11<00:02, 20.24it/s]\u001b[A\n",
            " 84%|████████▍ | 303/360 [00:11<00:03, 16.42it/s]\u001b[A\n",
            " 85%|████████▍ | 305/360 [00:12<00:03, 16.10it/s]\u001b[A\n",
            " 85%|████████▌ | 307/360 [00:12<00:03, 16.57it/s]\u001b[A\n",
            " 86%|████████▌ | 309/360 [00:12<00:03, 16.44it/s]\u001b[A\n",
            " 86%|████████▋ | 311/360 [00:12<00:03, 16.14it/s]\u001b[A\n",
            " 87%|████████▋ | 313/360 [00:12<00:03, 15.45it/s]\u001b[A\n",
            " 88%|████████▊ | 315/360 [00:12<00:02, 16.10it/s]\u001b[A\n",
            " 88%|████████▊ | 317/360 [00:12<00:02, 16.48it/s]\u001b[A\n",
            " 89%|████████▉ | 320/360 [00:12<00:02, 17.96it/s]\u001b[A\n",
            " 89%|████████▉ | 322/360 [00:13<00:02, 16.96it/s]\u001b[A\n",
            " 90%|█████████ | 324/360 [00:13<00:02, 17.42it/s]\u001b[A\n",
            " 91%|█████████ | 326/360 [00:13<00:01, 17.55it/s]\u001b[A\n",
            " 91%|█████████ | 328/360 [00:13<00:01, 17.53it/s]\u001b[A\n",
            " 92%|█████████▏| 330/360 [00:13<00:01, 17.22it/s]\u001b[A\n",
            " 92%|█████████▎| 333/360 [00:13<00:01, 18.68it/s]\u001b[A\n",
            " 93%|█████████▎| 336/360 [00:13<00:01, 20.09it/s]\u001b[A\n",
            " 94%|█████████▍| 338/360 [00:13<00:01, 19.60it/s]\u001b[A\n",
            " 94%|█████████▍| 340/360 [00:14<00:01, 19.39it/s]\u001b[A\n",
            " 95%|█████████▌| 343/360 [00:14<00:00, 19.97it/s]\u001b[A\n",
            " 96%|█████████▋| 347/360 [00:14<00:00, 23.88it/s]\u001b[A\n",
            " 98%|█████████▊| 353/360 [00:14<00:00, 32.05it/s]\u001b[A\n",
            "100%|██████████| 360/360 [00:15<00:00, 23.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inside calculate_balanced_accuracy, 3 classes passed in\n",
            "class0 recall: 0.0\n",
            "class1 recall: 0.0\n",
            "class2 recall: 1.0\n",
            "Inside calculate_balanced_accuracy, 3 classes passed in\n",
            "class0 recall: 0.0\n",
            "class1 recall: 0.0\n",
            "class2 recall: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "INFO:__main__:val progression view:\n",
            "INFO:__main__:At RAW Best val, validation/test/train 33.33 33.33 33.33\n",
            "INFO:__main__:At EMA Best val, validation/test/train 33.33 33.33 33.33\n",
            "100%|██████████| 1/1 [01:07<00:00, 67.77s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Type 2 - Image-Level Pre Training\n",
        "\n",
        "The arguments below train the SAMIL model with \"Image-Level\" Pre Training. This is achieved by passing in `FeatureExtractor1` as the value for the `Pretrained` argument."
      ],
      "metadata": {
        "id": "etMmxo2eU3-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RUNS_DIR = '/content/runs/'\n",
        "\n",
        "args = {\n",
        "    'training_seed': 0,\n",
        "    'Pretrained': 'FeatureExtractor1', # Options are Whole for Study Level, FeatureExtractor1 for Image Level, or NoPretail for no SSL.\n",
        "    'data_seed': 0,\n",
        "    'checkpoint_dir': MODEL_CHECKPOINTS,\n",
        "    'MIL_checkpoint_path': '',\n",
        "    'use_class_weights': 'True',\n",
        "    'ViewRegularization_warmup_schedule_type': 'Linear',\n",
        "    'optimizer_type': 'SGD',\n",
        "    'lr_schedule_type': 'CosineLR',\n",
        "    'lr_cycle_epochs': 1,\n",
        "    'lr': 0.0008, # learning rate\n",
        "    'wd': 0.0001, # weight decay\n",
        "    'T': 0.1, # tempertature\n",
        "    'lambda_ViewRegularization': 15.0, # λsA\n",
        "    'train_dir': RUNS_DIR + 'SAMIL',\n",
        "    'resume': 'last_checkpoint.pth.tar',\n",
        "    'dataset_name': 'echo',\n",
        "    'train_epoch': 1, # number of epochs, 2000 defined in the paper. CHANGE ME!\n",
        "    'development_size': 'DEV479',\n",
        "    'lr_warmup_epochs': 0,\n",
        "    'ema_decay': 0.999,\n",
        "    'device': device,\n",
        "    'start_epoch': 0,\n",
        "    'patience': 200,\n",
        "    'early_stopping_warmup': 200,\n",
        "    'ViewRegularization_warmup_pos': 0.4,\n",
        "    'eval_every_Xepoch': 1\n",
        "}\n",
        "\n",
        "args, brief_summary = setup_samil_train(args)\n",
        "\n",
        "weights = args['class_weights']\n",
        "weights = [float(i) for i in weights.split(',')]\n",
        "weights = torch.Tensor(weights)\n",
        "weights = weights.to(device)\n",
        "\n",
        "#load the view model, the output is unnormalized logits, need to use softmax on the output\n",
        "view_model = create_view_model(args)\n",
        "view_model.to(device)\n",
        "\n",
        "model = create_model(args)\n",
        "model.to(device)\n",
        "\n",
        "no_decay = ['bias', 'bn']\n",
        "grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(\n",
        "        nd in n for nd in no_decay)], 'weight_decay': args['wd']},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(\n",
        "        nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "\n",
        "if args['optimizer_type'] == 'SGD':\n",
        "    optimizer = optim.SGD(grouped_parameters, lr=args['lr'],\n",
        "                          momentum=0.9, nesterov=True)\n",
        "\n",
        "elif args['optimizer_type'] == 'Adam':\n",
        "    optimizer = optim.Adam(grouped_parameters, lr=args['lr'])\n",
        "\n",
        "elif args['optimizer_type'] == 'AdamW':\n",
        "    optimizer = optim.AdamW(grouped_parameters, lr=args['lr'])\n",
        "\n",
        "else:\n",
        "    raise NameError('Not supported optimizer setting')\n",
        "\n",
        "#lr_schedule_type choice\n",
        "if args['lr_schedule_type'] == 'CosineLR':\n",
        "    scheduler = get_cosine_schedule_with_warmup(optimizer, args['lr_warmup_epochs'], args['lr_cycle_epochs'])\n",
        "\n",
        "elif args['lr_schedule_type'] == 'FixedLR':\n",
        "    scheduler = get_fixed_lr(optimizer, args['lr_warmup_epochs'], args['lr_cycle_epochs'])\n",
        "\n",
        "else:\n",
        "    raise NameError('Not supported lr scheduler setting')\n",
        "\n",
        "\n",
        "#instantiate the ema_model object\n",
        "ema_model = ModelEMA(args, model, args['ema_decay'])\n",
        "\n",
        "\n",
        "# !!! Start training\n",
        "train_samil(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tH39gA40U3rN",
        "outputId": "f73cbe4d-9614-464b-9ce1-e283561f4f1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "setting training seed0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Model: WideResNet 28x2\n",
            "INFO:__main__:Total params for View Model: 5.93M\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!!!!!!!Using pre-calculated class weights!!!!!!!!\n",
            "args.resume_checkpoint_fullpath: /content/runs/SAMIL/FeatureExtractor1/last_checkpoint.pth.tar\n",
            "!!!!!!!!!!!!!!!!!!!!!initializing from pretrained checkpoint!!!!!!!!!!!!!!!!!!!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Total params: 2.31M\n",
            "INFO:__main__:***** Running training *****\n",
            "INFO:__main__:  Task = echo\n",
            "INFO:__main__:  Num Epochs = 1\n",
            "INFO:__main__:  Total optimization steps = 360\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "self.param_keys: ['feature_extractor_part1.0.weight', 'feature_extractor_part1.0.bias', 'feature_extractor_part1.3.weight', 'feature_extractor_part1.3.bias', 'feature_extractor_part1.6.weight', 'feature_extractor_part1.6.bias', 'feature_extractor_part1.9.weight', 'feature_extractor_part1.9.bias', 'feature_extractor_part2.0.weight', 'feature_extractor_part2.0.bias', 'feature_extractor_part3.0.weight', 'feature_extractor_part3.0.bias', 'feature_extractor_part3.2.weight', 'feature_extractor_part3.2.bias', 'attention_V.0.weight', 'attention_V.0.bias', 'attention_V.2.weight', 'attention_V.2.bias', 'attention_U.0.weight', 'attention_U.0.bias', 'attention_U.2.weight', 'attention_U.2.bias', 'classifier.0.weight', 'classifier.0.bias']\n",
            "self.buffer_keys: []\n",
            "!!!!Does not have checkpoint yet!!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/1 [00:00<?, ?it/s]\n",
            "  0%|          | 0/360 [00:42<?, ?it/s]\n",
            "\n",
            "  0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/119 [00:00<01:23,  1.41it/s]\u001b[A\n",
            "  2%|▏         | 2/119 [00:00<00:43,  2.67it/s]\u001b[A\n",
            "  3%|▎         | 4/119 [00:01<00:24,  4.71it/s]\u001b[A\n",
            "  6%|▌         | 7/119 [00:01<00:12,  8.93it/s]\u001b[A\n",
            "  8%|▊         | 9/119 [00:01<00:11,  9.45it/s]\u001b[A\n",
            " 10%|█         | 12/119 [00:01<00:08, 12.61it/s]\u001b[A\n",
            " 13%|█▎        | 15/119 [00:01<00:06, 14.88it/s]\u001b[A\n",
            " 14%|█▍        | 17/119 [00:01<00:06, 15.82it/s]\u001b[A\n",
            " 17%|█▋        | 20/119 [00:01<00:05, 17.33it/s]\u001b[A\n",
            " 19%|█▉        | 23/119 [00:02<00:05, 18.65it/s]\u001b[A\n",
            " 22%|██▏       | 26/119 [00:02<00:04, 21.13it/s]\u001b[A\n",
            " 24%|██▍       | 29/119 [00:02<00:04, 21.03it/s]\u001b[A\n",
            " 28%|██▊       | 33/119 [00:02<00:03, 23.58it/s]\u001b[A\n",
            " 30%|███       | 36/119 [00:02<00:03, 24.60it/s]\u001b[A\n",
            " 33%|███▎      | 39/119 [00:02<00:03, 21.62it/s]\u001b[A\n",
            " 35%|███▌      | 42/119 [00:03<00:04, 15.54it/s]\u001b[A\n",
            " 37%|███▋      | 44/119 [00:03<00:04, 15.32it/s]\u001b[A\n",
            " 39%|███▉      | 47/119 [00:03<00:04, 17.21it/s]\u001b[A\n",
            " 41%|████      | 49/119 [00:03<00:04, 15.83it/s]\u001b[A\n",
            " 43%|████▎     | 51/119 [00:03<00:04, 13.82it/s]\u001b[A\n",
            " 45%|████▌     | 54/119 [00:03<00:04, 15.64it/s]\u001b[A\n",
            " 48%|████▊     | 57/119 [00:03<00:03, 17.96it/s]\u001b[A\n",
            " 50%|█████     | 60/119 [00:04<00:03, 19.39it/s]\u001b[A\n",
            " 53%|█████▎    | 63/119 [00:04<00:03, 16.75it/s]\u001b[A\n",
            " 55%|█████▍    | 65/119 [00:04<00:03, 17.00it/s]\u001b[A\n",
            " 56%|█████▋    | 67/119 [00:04<00:03, 15.36it/s]\u001b[A\n",
            " 59%|█████▉    | 70/119 [00:04<00:02, 17.26it/s]\u001b[A\n",
            " 61%|██████    | 72/119 [00:04<00:02, 17.29it/s]\u001b[A\n",
            " 62%|██████▏   | 74/119 [00:04<00:02, 17.39it/s]\u001b[A\n",
            " 65%|██████▍   | 77/119 [00:05<00:02, 17.79it/s]\u001b[A\n",
            " 66%|██████▋   | 79/119 [00:05<00:02, 16.70it/s]\u001b[A\n",
            " 68%|██████▊   | 81/119 [00:05<00:02, 17.10it/s]\u001b[A\n",
            " 70%|██████▉   | 83/119 [00:05<00:02, 17.45it/s]\u001b[A\n",
            " 71%|███████▏  | 85/119 [00:05<00:02, 16.92it/s]\u001b[A\n",
            " 73%|███████▎  | 87/119 [00:05<00:01, 16.85it/s]\u001b[A\n",
            " 75%|███████▍  | 89/119 [00:05<00:01, 17.02it/s]\u001b[A\n",
            " 76%|███████▋  | 91/119 [00:06<00:01, 14.33it/s]\u001b[A\n",
            " 79%|███████▉  | 94/119 [00:06<00:01, 15.28it/s]\u001b[A\n",
            " 81%|████████  | 96/119 [00:06<00:01, 15.55it/s]\u001b[A\n",
            " 82%|████████▏ | 98/119 [00:06<00:01, 14.65it/s]\u001b[A\n",
            " 84%|████████▍ | 100/119 [00:06<00:01, 15.51it/s]\u001b[A\n",
            " 87%|████████▋ | 103/119 [00:06<00:00, 17.31it/s]\u001b[A\n",
            " 89%|████████▉ | 106/119 [00:06<00:00, 19.09it/s]\u001b[A\n",
            " 92%|█████████▏| 110/119 [00:06<00:00, 23.32it/s]\u001b[A\n",
            "100%|██████████| 119/119 [00:07<00:00, 15.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inside calculate_balanced_accuracy, 3 classes passed in\n",
            "class0 recall: 0.0\n",
            "class1 recall: 0.0\n",
            "class2 recall: 1.0\n",
            "Inside calculate_balanced_accuracy, 3 classes passed in\n",
            "class0 recall: 0.0\n",
            "class1 recall: 0.0\n",
            "class2 recall: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0/120 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/120 [00:00<01:42,  1.16it/s]\u001b[A\n",
            "  2%|▏         | 2/120 [00:01<00:50,  2.32it/s]\u001b[A\n",
            "  2%|▎         | 3/120 [00:01<00:38,  3.07it/s]\u001b[A\n",
            "  4%|▍         | 5/120 [00:01<00:20,  5.62it/s]\u001b[A\n",
            "  6%|▌         | 7/120 [00:01<00:13,  8.08it/s]\u001b[A\n",
            "  8%|▊         | 10/120 [00:01<00:08, 12.25it/s]\u001b[A\n",
            " 11%|█         | 13/120 [00:01<00:06, 15.79it/s]\u001b[A\n",
            " 13%|█▎        | 16/120 [00:01<00:06, 17.12it/s]\u001b[A\n",
            " 17%|█▋        | 20/120 [00:01<00:04, 21.31it/s]\u001b[A\n",
            " 20%|██        | 24/120 [00:02<00:03, 25.32it/s]\u001b[A\n",
            " 22%|██▎       | 27/120 [00:02<00:04, 22.17it/s]\u001b[A\n",
            " 25%|██▌       | 30/120 [00:02<00:04, 22.14it/s]\u001b[A\n",
            " 28%|██▊       | 33/120 [00:02<00:03, 23.35it/s]\u001b[A\n",
            " 30%|███       | 36/120 [00:02<00:03, 24.88it/s]\u001b[A\n",
            " 32%|███▎      | 39/120 [00:02<00:03, 24.92it/s]\u001b[A\n",
            " 36%|███▌      | 43/120 [00:02<00:02, 26.43it/s]\u001b[A\n",
            " 38%|███▊      | 46/120 [00:02<00:02, 27.27it/s]\u001b[A\n",
            " 41%|████      | 49/120 [00:03<00:02, 27.87it/s]\u001b[A\n",
            " 43%|████▎     | 52/120 [00:03<00:02, 28.40it/s]\u001b[A\n",
            " 46%|████▌     | 55/120 [00:03<00:02, 24.82it/s]\u001b[A\n",
            " 48%|████▊     | 58/120 [00:03<00:02, 24.42it/s]\u001b[A\n",
            " 51%|█████     | 61/120 [00:03<00:02, 24.93it/s]\u001b[A\n",
            " 53%|█████▎    | 64/120 [00:03<00:02, 24.95it/s]\u001b[A\n",
            " 57%|█████▊    | 69/120 [00:03<00:01, 29.41it/s]\u001b[A\n",
            " 60%|██████    | 72/120 [00:03<00:01, 24.94it/s]\u001b[A\n",
            " 62%|██████▎   | 75/120 [00:04<00:01, 24.08it/s]\u001b[A\n",
            " 65%|██████▌   | 78/120 [00:04<00:01, 23.91it/s]\u001b[A\n",
            " 68%|██████▊   | 81/120 [00:04<00:01, 23.21it/s]\u001b[A\n",
            " 70%|███████   | 84/120 [00:04<00:01, 22.40it/s]\u001b[A\n",
            " 72%|███████▎  | 87/120 [00:04<00:01, 20.65it/s]\u001b[A\n",
            " 76%|███████▌  | 91/120 [00:04<00:01, 23.99it/s]\u001b[A\n",
            " 78%|███████▊  | 94/120 [00:04<00:01, 23.63it/s]\u001b[A\n",
            " 81%|████████  | 97/120 [00:05<00:00, 23.74it/s]\u001b[A\n",
            " 83%|████████▎ | 100/120 [00:05<00:00, 21.59it/s]\u001b[A\n",
            " 87%|████████▋ | 104/120 [00:05<00:00, 24.64it/s]\u001b[A\n",
            " 92%|█████████▎| 111/120 [00:05<00:00, 34.78it/s]\u001b[A\n",
            "100%|██████████| 120/120 [00:06<00:00, 19.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inside calculate_balanced_accuracy, 3 classes passed in\n",
            "class0 recall: 0.0\n",
            "class1 recall: 0.0\n",
            "class2 recall: 1.0\n",
            "Inside calculate_balanced_accuracy, 3 classes passed in\n",
            "class0 recall: 0.0\n",
            "class1 recall: 0.0\n",
            "class2 recall: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0/360 [00:00<?, ?it/s]\u001b[A\n",
            "  0%|          | 1/360 [00:00<04:51,  1.23it/s]\u001b[A\n",
            "  1%|          | 2/360 [00:01<02:50,  2.10it/s]\u001b[A\n",
            "  1%|          | 3/360 [00:01<01:56,  3.07it/s]\u001b[A\n",
            "  2%|▏         | 6/360 [00:01<00:47,  7.38it/s]\u001b[A\n",
            "  2%|▏         | 8/360 [00:01<00:37,  9.51it/s]\u001b[A\n",
            "  3%|▎         | 10/360 [00:01<00:31, 11.02it/s]\u001b[A\n",
            "  4%|▎         | 13/360 [00:01<00:24, 14.44it/s]\u001b[A\n",
            "  4%|▍         | 16/360 [00:01<00:19, 17.98it/s]\u001b[A\n",
            "  5%|▌         | 19/360 [00:01<00:17, 19.21it/s]\u001b[A\n",
            "  6%|▋         | 23/360 [00:02<00:14, 23.87it/s]\u001b[A\n",
            "  7%|▋         | 26/360 [00:02<00:13, 25.19it/s]\u001b[A\n",
            "  8%|▊         | 29/360 [00:02<00:12, 26.42it/s]\u001b[A\n",
            "  9%|▉         | 32/360 [00:02<00:12, 26.32it/s]\u001b[A\n",
            " 10%|▉         | 35/360 [00:02<00:12, 25.51it/s]\u001b[A\n",
            " 11%|█         | 38/360 [00:02<00:13, 23.90it/s]\u001b[A\n",
            " 12%|█▏        | 42/360 [00:02<00:11, 26.93it/s]\u001b[A\n",
            " 13%|█▎        | 46/360 [00:02<00:10, 28.78it/s]\u001b[A\n",
            " 14%|█▎        | 49/360 [00:02<00:11, 28.23it/s]\u001b[A\n",
            " 15%|█▍        | 53/360 [00:03<00:10, 29.35it/s]\u001b[A\n",
            " 16%|█▌        | 56/360 [00:03<00:10, 28.78it/s]\u001b[A\n",
            " 16%|█▋        | 59/360 [00:03<00:11, 26.21it/s]\u001b[A\n",
            " 18%|█▊        | 64/360 [00:03<00:09, 29.81it/s]\u001b[A\n",
            " 19%|█▊        | 67/360 [00:03<00:12, 23.97it/s]\u001b[A\n",
            " 19%|█▉        | 70/360 [00:03<00:12, 23.23it/s]\u001b[A\n",
            " 20%|██        | 73/360 [00:04<00:16, 17.38it/s]\u001b[A\n",
            " 21%|██        | 76/360 [00:04<00:16, 17.65it/s]\u001b[A\n",
            " 22%|██▏       | 78/360 [00:04<00:16, 16.73it/s]\u001b[A\n",
            " 22%|██▏       | 80/360 [00:04<00:17, 15.79it/s]\u001b[A\n",
            " 23%|██▎       | 82/360 [00:04<00:17, 15.63it/s]\u001b[A\n",
            " 23%|██▎       | 84/360 [00:04<00:19, 14.10it/s]\u001b[A\n",
            " 24%|██▍       | 86/360 [00:05<00:22, 12.41it/s]\u001b[A\n",
            " 24%|██▍       | 88/360 [00:05<00:21, 12.64it/s]\u001b[A\n",
            " 25%|██▌       | 91/360 [00:05<00:17, 15.34it/s]\u001b[A\n",
            " 26%|██▌       | 93/360 [00:05<00:17, 14.95it/s]\u001b[A\n",
            " 26%|██▋       | 95/360 [00:05<00:18, 14.53it/s]\u001b[A\n",
            " 27%|██▋       | 97/360 [00:05<00:17, 14.95it/s]\u001b[A\n",
            " 28%|██▊       | 99/360 [00:05<00:20, 12.96it/s]\u001b[A\n",
            " 28%|██▊       | 102/360 [00:06<00:17, 15.01it/s]\u001b[A\n",
            " 29%|██▉       | 104/360 [00:06<00:18, 14.04it/s]\u001b[A\n",
            " 29%|██▉       | 106/360 [00:06<00:17, 14.58it/s]\u001b[A\n",
            " 30%|███       | 108/360 [00:06<00:17, 14.19it/s]\u001b[A\n",
            " 31%|███       | 110/360 [00:06<00:17, 14.04it/s]\u001b[A\n",
            " 31%|███       | 112/360 [00:06<00:18, 13.54it/s]\u001b[A\n",
            " 32%|███▏      | 114/360 [00:06<00:16, 14.85it/s]\u001b[A\n",
            " 32%|███▎      | 117/360 [00:07<00:13, 17.87it/s]\u001b[A\n",
            " 33%|███▎      | 120/360 [00:07<00:12, 18.93it/s]\u001b[A\n",
            " 34%|███▍      | 122/360 [00:07<00:13, 17.92it/s]\u001b[A\n",
            " 34%|███▍      | 124/360 [00:07<00:12, 18.31it/s]\u001b[A\n",
            " 35%|███▌      | 126/360 [00:07<00:13, 16.86it/s]\u001b[A\n",
            " 36%|███▌      | 128/360 [00:07<00:13, 17.29it/s]\u001b[A\n",
            " 36%|███▌      | 130/360 [00:07<00:14, 16.12it/s]\u001b[A\n",
            " 37%|███▋      | 132/360 [00:08<00:14, 15.90it/s]\u001b[A\n",
            " 37%|███▋      | 134/360 [00:08<00:13, 16.28it/s]\u001b[A\n",
            " 38%|███▊      | 136/360 [00:08<00:13, 16.66it/s]\u001b[A\n",
            " 38%|███▊      | 138/360 [00:08<00:13, 16.74it/s]\u001b[A\n",
            " 39%|███▉      | 140/360 [00:08<00:13, 16.82it/s]\u001b[A\n",
            " 40%|███▉      | 143/360 [00:08<00:11, 19.45it/s]\u001b[A\n",
            " 41%|████      | 146/360 [00:08<00:10, 21.17it/s]\u001b[A\n",
            " 41%|████▏     | 149/360 [00:08<00:09, 22.74it/s]\u001b[A\n",
            " 42%|████▏     | 152/360 [00:08<00:09, 21.05it/s]\u001b[A\n",
            " 43%|████▎     | 155/360 [00:09<00:09, 21.80it/s]\u001b[A\n",
            " 44%|████▍     | 158/360 [00:09<00:09, 21.31it/s]\u001b[A\n",
            " 45%|████▍     | 161/360 [00:09<00:09, 20.06it/s]\u001b[A\n",
            " 46%|████▌     | 164/360 [00:09<00:09, 20.34it/s]\u001b[A\n",
            " 46%|████▋     | 167/360 [00:09<00:11, 16.64it/s]\u001b[A\n",
            " 47%|████▋     | 170/360 [00:09<00:10, 18.92it/s]\u001b[A\n",
            " 48%|████▊     | 173/360 [00:10<00:09, 19.62it/s]\u001b[A\n",
            " 49%|████▉     | 176/360 [00:10<00:08, 21.27it/s]\u001b[A\n",
            " 50%|█████     | 180/360 [00:10<00:07, 24.01it/s]\u001b[A\n",
            " 51%|█████     | 183/360 [00:10<00:07, 24.12it/s]\u001b[A\n",
            " 52%|█████▏    | 187/360 [00:10<00:06, 27.94it/s]\u001b[A\n",
            " 53%|█████▎    | 190/360 [00:10<00:06, 26.34it/s]\u001b[A\n",
            " 54%|█████▎    | 193/360 [00:10<00:06, 27.07it/s]\u001b[A\n",
            " 54%|█████▍    | 196/360 [00:10<00:06, 23.49it/s]\u001b[A\n",
            " 56%|█████▌    | 200/360 [00:11<00:06, 26.48it/s]\u001b[A\n",
            " 56%|█████▋    | 203/360 [00:11<00:06, 23.27it/s]\u001b[A\n",
            " 57%|█████▊    | 207/360 [00:11<00:05, 26.30it/s]\u001b[A\n",
            " 58%|█████▊    | 210/360 [00:11<00:06, 23.67it/s]\u001b[A\n",
            " 59%|█████▉    | 213/360 [00:11<00:06, 23.61it/s]\u001b[A\n",
            " 60%|██████    | 216/360 [00:11<00:05, 24.10it/s]\u001b[A\n",
            " 61%|██████    | 219/360 [00:11<00:05, 23.96it/s]\u001b[A\n",
            " 62%|██████▏   | 222/360 [00:11<00:05, 23.69it/s]\u001b[A\n",
            " 62%|██████▎   | 225/360 [00:12<00:06, 22.04it/s]\u001b[A\n",
            " 63%|██████▎   | 228/360 [00:12<00:06, 20.02it/s]\u001b[A\n",
            " 64%|██████▍   | 231/360 [00:12<00:06, 21.28it/s]\u001b[A\n",
            " 65%|██████▌   | 234/360 [00:12<00:06, 20.67it/s]\u001b[A\n",
            " 66%|██████▌   | 237/360 [00:12<00:05, 22.18it/s]\u001b[A\n",
            " 67%|██████▋   | 241/360 [00:12<00:05, 23.52it/s]\u001b[A\n",
            " 68%|██████▊   | 244/360 [00:13<00:05, 22.58it/s]\u001b[A\n",
            " 69%|██████▊   | 247/360 [00:13<00:05, 22.49it/s]\u001b[A\n",
            " 69%|██████▉   | 250/360 [00:13<00:04, 23.18it/s]\u001b[A\n",
            " 70%|███████   | 253/360 [00:13<00:04, 22.71it/s]\u001b[A\n",
            " 71%|███████▏  | 257/360 [00:13<00:04, 25.00it/s]\u001b[A\n",
            " 72%|███████▏  | 260/360 [00:13<00:04, 24.70it/s]\u001b[A\n",
            " 73%|███████▎  | 264/360 [00:13<00:03, 27.92it/s]\u001b[A\n",
            " 74%|███████▍  | 267/360 [00:13<00:03, 25.12it/s]\u001b[A\n",
            " 75%|███████▌  | 270/360 [00:14<00:04, 21.59it/s]\u001b[A\n",
            " 76%|███████▌  | 273/360 [00:14<00:04, 20.84it/s]\u001b[A\n",
            " 77%|███████▋  | 276/360 [00:14<00:03, 21.48it/s]\u001b[A\n",
            " 78%|███████▊  | 279/360 [00:14<00:03, 21.11it/s]\u001b[A\n",
            " 78%|███████▊  | 282/360 [00:14<00:03, 21.51it/s]\u001b[A\n",
            " 79%|███████▉  | 285/360 [00:14<00:03, 21.99it/s]\u001b[A\n",
            " 80%|████████  | 289/360 [00:14<00:02, 24.48it/s]\u001b[A\n",
            " 81%|████████  | 292/360 [00:15<00:02, 23.11it/s]\u001b[A\n",
            " 82%|████████▏ | 295/360 [00:15<00:02, 23.27it/s]\u001b[A\n",
            " 83%|████████▎ | 298/360 [00:15<00:02, 21.65it/s]\u001b[A\n",
            " 84%|████████▎ | 301/360 [00:15<00:02, 19.93it/s]\u001b[A\n",
            " 84%|████████▍ | 304/360 [00:15<00:02, 19.89it/s]\u001b[A\n",
            " 86%|████████▌ | 308/360 [00:15<00:02, 22.55it/s]\u001b[A\n",
            " 86%|████████▋ | 311/360 [00:16<00:02, 20.41it/s]\u001b[A\n",
            " 87%|████████▋ | 314/360 [00:16<00:02, 17.65it/s]\u001b[A\n",
            " 88%|████████▊ | 316/360 [00:16<00:02, 17.95it/s]\u001b[A\n",
            " 88%|████████▊ | 318/360 [00:16<00:02, 17.93it/s]\u001b[A\n",
            " 89%|████████▉ | 321/360 [00:16<00:02, 19.39it/s]\u001b[A\n",
            " 90%|█████████ | 324/360 [00:16<00:01, 20.72it/s]\u001b[A\n",
            " 91%|█████████ | 327/360 [00:16<00:01, 22.34it/s]\u001b[A\n",
            " 92%|█████████▏| 330/360 [00:17<00:01, 20.15it/s]\u001b[A\n",
            " 92%|█████████▎| 333/360 [00:17<00:01, 21.09it/s]\u001b[A\n",
            " 93%|█████████▎| 336/360 [00:17<00:01, 22.83it/s]\u001b[A\n",
            " 94%|█████████▍| 339/360 [00:17<00:00, 23.32it/s]\u001b[A\n",
            " 95%|█████████▌| 342/360 [00:17<00:00, 23.47it/s]\u001b[A\n",
            " 96%|█████████▌| 346/360 [00:17<00:00, 26.68it/s]\u001b[A\n",
            " 98%|█████████▊| 351/360 [00:17<00:00, 32.10it/s]\u001b[A\n",
            " 99%|█████████▊| 355/360 [00:17<00:00, 34.03it/s]\u001b[A\n",
            "100%|██████████| 360/360 [00:18<00:00, 19.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inside calculate_balanced_accuracy, 3 classes passed in\n",
            "class0 recall: 0.0\n",
            "class1 recall: 0.0\n",
            "class2 recall: 1.0\n",
            "Inside calculate_balanced_accuracy, 3 classes passed in\n",
            "class0 recall: 0.0\n",
            "class1 recall: 0.0\n",
            "class2 recall: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "INFO:__main__:val progression view:\n",
            "INFO:__main__:At RAW Best val, validation/test/train 33.33 33.33 33.33\n",
            "INFO:__main__:At EMA Best val, validation/test/train 33.33 33.33 33.33\n",
            "100%|██████████| 1/1 [01:16<00:00, 76.14s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training Type 3 - Study-Level (i.e., Bag-Level) Pre Training\n",
        "\n",
        "This training run trains the model with the two novel contributions of the paper: 1) the supervised-attention mechanism, and 2) the self-supervised (SSL) pre-training of the entire **study-level representations** that builds upon MoCo (V2). MoCo is a \"recent method for self-supervised image-level contrastive learning (img-CL). (Huang et., al, 2023).\n",
        "\n",
        "The arguments below specify the epoch count (`train_epoch`), the Hyperparameters (`lr`, `wd`, `T`, and `lambda_ViewRegularization`). Most importanly, it species `Pretrained` argument as `Whole` which represents the Study Level pretraining."
      ],
      "metadata": {
        "id": "bY8qY5rl9cHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RUNS_DIR = '/content/runs/'\n",
        "\n",
        "args = {\n",
        "    'training_seed': 0,\n",
        "    'Pretrained': 'Whole', # Options are Whole for Study Level, FeatureExtractor for Image Level, or NoPretail for no SSL.\n",
        "    'data_seed': 0,\n",
        "    'checkpoint_dir': MODEL_CHECKPOINTS,\n",
        "    'MIL_checkpoint_path': '',\n",
        "    'use_class_weights': 'True',\n",
        "    'ViewRegularization_warmup_schedule_type': 'Linear',\n",
        "    'optimizer_type': 'SGD',\n",
        "    'lr_schedule_type': 'CosineLR',\n",
        "    'lr_cycle_epochs': 1,\n",
        "    'lr': 0.0008, # learning rate\n",
        "    'wd': 0.0001, # weight decay\n",
        "    'T': 0.1, # tempertature\n",
        "    'lambda_ViewRegularization': 15.0, # λsA\n",
        "    'train_dir': RUNS_DIR + 'SAMIL',\n",
        "    'resume': 'last_checkpoint.pth.tar',\n",
        "    'dataset_name': 'echo',\n",
        "    'train_epoch': 1, # number of epochs, 2000 defined in the paper. CHANGE ME!\n",
        "    'development_size': 'DEV479',\n",
        "    'lr_warmup_epochs': 0,\n",
        "    'ema_decay': 0.999,\n",
        "    'device': device,\n",
        "    'start_epoch': 0,\n",
        "    'patience': 200,\n",
        "    'early_stopping_warmup': 200,\n",
        "    'ViewRegularization_warmup_pos': 0.4,\n",
        "    'eval_every_Xepoch': 1\n",
        "}\n",
        "\n",
        "args, brief_summary = setup_samil_train(args)\n",
        "\n",
        "weights = args['class_weights']\n",
        "weights = [float(i) for i in weights.split(',')]\n",
        "weights = torch.Tensor(weights)\n",
        "weights = weights.to(device)\n",
        "\n",
        "#load the view model, the output is unnormalized logits, need to use softmax on the output\n",
        "view_model = create_view_model(args)\n",
        "view_model.to(device)\n",
        "\n",
        "model = create_model(args)\n",
        "model.to(device)\n",
        "\n",
        "no_decay = ['bias', 'bn']\n",
        "grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(\n",
        "        nd in n for nd in no_decay)], 'weight_decay': args['wd']},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(\n",
        "        nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "\n",
        "if args['optimizer_type'] == 'SGD':\n",
        "    optimizer = optim.SGD(grouped_parameters, lr=args['lr'],\n",
        "                          momentum=0.9, nesterov=True)\n",
        "\n",
        "elif args['optimizer_type'] == 'Adam':\n",
        "    optimizer = optim.Adam(grouped_parameters, lr=args['lr'])\n",
        "\n",
        "elif args['optimizer_type'] == 'AdamW':\n",
        "    optimizer = optim.AdamW(grouped_parameters, lr=args['lr'])\n",
        "\n",
        "else:\n",
        "    raise NameError('Not supported optimizer setting')\n",
        "\n",
        "#lr_schedule_type choice\n",
        "if args['lr_schedule_type'] == 'CosineLR':\n",
        "    scheduler = get_cosine_schedule_with_warmup(optimizer, args['lr_warmup_epochs'], args['lr_cycle_epochs'])\n",
        "\n",
        "elif args['lr_schedule_type'] == 'FixedLR':\n",
        "    scheduler = get_fixed_lr(optimizer, args['lr_warmup_epochs'], args['lr_cycle_epochs'])\n",
        "\n",
        "else:\n",
        "    raise NameError('Not supported lr scheduler setting')\n",
        "\n",
        "\n",
        "#instantiate the ema_model object\n",
        "ema_model = ModelEMA(args, model, args['ema_decay'])\n",
        "\n",
        "\n",
        "# !!! Start training\n",
        "train_samil(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pE_QfyV89b1N",
        "outputId": "c1dba679-d817-4ce6-f17c-0b9a16087cb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "setting training seed0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Model: WideResNet 28x2\n",
            "INFO:__main__:Total params for View Model: 5.93M\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!!!!!!!Using pre-calculated class weights!!!!!!!!\n",
            "args.resume_checkpoint_fullpath: /content/runs/SAMIL/Whole/last_checkpoint.pth.tar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Total params: 2.31M\n",
            "INFO:__main__:***** Running training *****\n",
            "INFO:__main__:  Task = echo\n",
            "INFO:__main__:  Num Epochs = 1\n",
            "INFO:__main__:  Total optimization steps = 360\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!!!!!!!!!!!!!!!!!!!!initializing from pretrained checkpoint!!!!!!!!!!!!!!!!!!!!!\n",
            "self.param_keys: ['feature_extractor_part1.0.weight', 'feature_extractor_part1.0.bias', 'feature_extractor_part1.3.weight', 'feature_extractor_part1.3.bias', 'feature_extractor_part1.6.weight', 'feature_extractor_part1.6.bias', 'feature_extractor_part1.9.weight', 'feature_extractor_part1.9.bias', 'feature_extractor_part2.0.weight', 'feature_extractor_part2.0.bias', 'feature_extractor_part3.0.weight', 'feature_extractor_part3.0.bias', 'feature_extractor_part3.2.weight', 'feature_extractor_part3.2.bias', 'attention_V.0.weight', 'attention_V.0.bias', 'attention_V.2.weight', 'attention_V.2.bias', 'attention_U.0.weight', 'attention_U.0.bias', 'attention_U.2.weight', 'attention_U.2.bias', 'classifier.0.weight', 'classifier.0.bias']\n",
            "self.buffer_keys: []\n",
            "Resuming from checkpoint: /content/runs/SAMIL/Whole/last_checkpoint.pth.tar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n",
        "\n",
        "The sections below show the balanced accuracy scores after training for each of the models compared to the paper.\n",
        "\n",
        "At the time of draft submission, only the SAMIL w/ Study Level Pretraining has been trained."
      ],
      "metadata": {
        "id": "gX6bCcZNuxmz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SAMIL with Study Level Pretraining\n",
        "\n",
        "The reproduction of the SAMIL model with Study Level Pretraining was trained for 446 epochs before Colab terminated the A100 GPU runtime. At that time, the best balanced accuracy achieved was 0.663 vs. the paper's 0.754. We will continue training from the checkpoint once a more stable environment has been chosen.\n",
        "\n",
        "The code block below compares the balanced accuracy of the SAMIL with Study Level Pre Training from the checkpoint file captured during training by downloading the checkpoint from Google Drive on a publicly available link using `gdown`."
      ],
      "metadata": {
        "id": "YGeSbUVS_TZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "target_balanced_accuracy = 0.754\n",
        "\n",
        "args = {\n",
        "    'training_seed': 0,\n",
        "    'Pretrained': 'Whole', # Options are Whole for Study Level, FeatureExtractor for Image Level, or NoPretail for no SSL.\n",
        "    'data_seed': 0,\n",
        "    'checkpoint_dir': MODEL_CHECKPOINTS,\n",
        "    'MIL_checkpoint_path': '',\n",
        "    'use_class_weights': 'True',\n",
        "    'ViewRegularization_warmup_schedule_type': 'Linear',\n",
        "    'optimizer_type': 'SGD',\n",
        "    'lr_schedule_type': 'CosineLR',\n",
        "    'lr_cycle_epochs': 1,\n",
        "    'lr': 0.0008, # learning rate\n",
        "    'wd': 0.0001, # weight decay\n",
        "    'T': 0.1, # tempertature\n",
        "    'lambda_ViewRegularization': 15.0, # λsA\n",
        "    'train_dir': RUNS_DIR + 'SAMIL',\n",
        "    'resume': 'last_checkpoint.pth.tar',\n",
        "    'dataset_name': 'echo',\n",
        "    'train_epoch': 2000, # number of epochs, 2000 defined in the paper. CHANGE ME!\n",
        "    'development_size': 'DEV479',\n",
        "    'lr_warmup_epochs': 0,\n",
        "    'ema_decay': 0.999,\n",
        "    'device': device,\n",
        "    'start_epoch': 0,\n",
        "    'patience': 200,\n",
        "    'early_stopping_warmup': 200,\n",
        "    'ViewRegularization_warmup_pos': 0.4,\n",
        "    'eval_every_Xepoch': 1\n",
        "}\n",
        "\n",
        "\n",
        "# Download model checkpoint\n",
        "!gdown 'https://drive.google.com/uc?id=1q4ROvzCUlZdfR1qArH1_G27BPiU-Zonz'\n",
        "model_checkpoint = os.path.join('', 'samil_bag_last_checkpoint.pth.tar')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHH1um4k_Y9Q",
        "outputId": "9f92e56f-5191-482b-f5d6-8466d9757337"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1q4ROvzCUlZdfR1qArH1_G27BPiU-Zonz\n",
            "To: /content/samil_bag_last_checkpoint.pth.tar\n",
            "100% 27.8M/27.8M [00:00<00:00, 63.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load(model_checkpoint)\n",
        "\n",
        "model = SAMIL().to(device)\n",
        "model.load_state_dict(torch.load(model_checkpoint)['state_dict'])\n",
        "\n",
        "model.eval()\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=8)\n",
        "repro_balanced_accuracy_score = eval_model_test(args, test_loader, model)\n",
        "\n",
        "print(f\"Target bal_acc (from paper): {target_balanced_accuracy}\")\n",
        "print(f\"Reproduced bal_acc (from repro): {repro_balanced_accuracy_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lko--hREbAOJ",
        "outputId": "4fbca29d-34c1-46c0-d718-3e2fd6d4bc6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target bal_acc (from paper): 0.754\n",
            "Reproduced bal_acc (from repro): 0.663298139768728\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "\n",
        "At the time of draft submission, the only result to report upon is the partial training of the SAMIL model with study-level pretraining. Training did not complete, but the reproduced balanced accuracy reached 0.663 after 446 epochs.\n",
        "\n",
        "| Variant | Reproduced Balanced Accuracy | Paper Balanced Accuracy |\n",
        "| ---------------------------- | ------ | ------ |\n",
        "| ABMIL               | TBD | 58.5 |\n",
        "| SAMIL w/ No Pretraining                 | TBD | 72.7 |\n",
        "| SAMIL w/ Image-Level Pretraining                | TBD    | 71.2   |\n",
        "| SAMIL w/ Study-Level Pretraining               | 63.3   | 75.4   |\n",
        "\n",
        "## Analyses\n",
        "\n",
        "No analyses conducted at this time."
      ],
      "metadata": {
        "id": "5L7kLVdLYx49"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion\n",
        "\n",
        "\n",
        "### Reproducibility\n",
        "\n",
        "At this time, it cannot be concluded whether the results in the paper can be reproduced, however it can be confirmed that the steps necessary to attempt reproducibility are feasible. The code in the paper's Github repo is usable, functional, and requires little to no modification to get started. The dataset, while difficult to obtail initially, is also maintained and accessible when access is acquired from the owners.\n",
        "\n",
        "\n",
        "### Challenges\n",
        "\n",
        "1. The code for this paper assumes you will run it outside of a Jupyter notebook, so refactoring of the classes was required to get this functional in a notebook.\n",
        "\n",
        "2. While the TMED-2 dataset has an access request form, it took three attempts over several weeks to gain access. Ultimately, emailing the staff on the TMED-2 website was required beyond their signup form. For individuals looking to reproduce the results of the paper in short notice, this might pose a challenge.\n",
        "\n",
        "3. The documentation for the paper's code in the repo was poor. There is only one example of running an experiment without making any changes to the parameters. The authors should have provided robust documentation in the Github repo for how to reproduce each model they evaluated in the various configurations.\n",
        "\n",
        "4. The computational requirements to train each model pose a challenge. Training on a standard GPU provided in colab is estimated to take 3-4 days to run the 2,000 epoch upperbound defined in the paper. A100 GPU units were purchased from Colab and enabled, which improved the speed of training, but on several occasions, the instance would get terminated by Colab/timeout. The reproducibility of this paper should only be considered feasible if individuals are willing to acquire GPUs such as the A100 through Colab/AWS/Azure/etc.\n",
        "\n",
        "5. Instructions in the Github repo readme were unclear/incorrect on how to load in the dataset. Modification of the data loading method was necessary to get this running based on the structure of the data from TMED-2. Perhaps this was a misunderstanding on my part, but I could not get this to work based on the original instructions.\n",
        "\n",
        "Despite the challenges above, once the code was refactored into a notebook and the A100 GPU was used in Colab, the code itself was functional without any bug fixing or adjustment to solve for out of date libraries.\n",
        "\n",
        "### Suggestions\n",
        "\n",
        "To improve reproducibility of this paper, the following suggestions are recommended:\n",
        "\n",
        "1. Update the Github repo readme to specify exactly how to structure the downloaded data from TMED-2 (where to put it, what the folder structure should be, etc.)\n",
        "\n",
        "2. TMED-2 dataset needs a better mechanism to acquire access in a faster more transparent way. The Google Form was filled out 3 times over 3 weeks with no response. An email to all the authors of the dataset was required to gain access and a response.\n",
        "\n",
        "3. Github repo readme should outline the steps to reproduce the exact results in the paper, step by step. The readme only gives one example, and no steps on what to adjust for each experiment run in the paper. In summary, for each experiment in the paper, there should be a 1:1 instruction in the readme.\n",
        "\n",
        "4. Paper author's should included the runtime of their model on the A100. They mentioned it ran on a single A100 in the paper, but no reference to how long for each run.\n",
        "\n",
        "### Future Plans\n",
        "\n",
        "The following work remains to complete the reproduction of this paper. Each item will be completed by the final submission.\n",
        "\n",
        "1. Include the ABMIL model class from the paper, train it, and include in the Results/Analysis section. This is one of the models evaluated by the paper.\n",
        "\n",
        "2. Fully train the SAMIL model with No Pretraining by moving to an A100 instance on Lambda Labs.\n",
        "\n",
        "3. Complete the following sections that were not completed in the draft, and the additional sections required in the final report:\n",
        "    * Results: Include final Balanced Accuracy results for all 4 models.\n",
        "    * Analysis: Complete an Analysis of the 4 models for the following metrics: Balanced Accuracy, Numer of Epochs before Early Stop. Analysis to include a chart of the values for each model, and graphs for Balanced Accuracy vs. Epoch, Loss vs. Epoch.\n",
        "    * Include a section for Environment setup (Python version, packages), Data visualizations, detailed information on the Hyperparams used by this notebook vs. in the paper, additional writeups on the models, and include a section for the Ablation Study.\n",
        "\n",
        "4. Simulate image data to enable an individual to run through the notebook without having access to the TMED-2 dataset.\n",
        "\n",
        "\n",
        "I kindly ask you to consider these Future Plans during the assessment of this draft. I have recently moved into a new role in my full time job and have been unable to dedicate sufficient time to this draft, in additional to hitting multiple blockers during training in Colab."
      ],
      "metadata": {
        "id": "qH75TNU71eRH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "@misc{huang2021new,\n",
        "      title={A New Semi-supervised Learning Benchmark for Classifying View and Diagnosing Aortic Stenosis from Echocardiograms},\n",
        "      author={Zhe Huang and Gary Long and Benjamin Wessler and Michael C. Hughes},\n",
        "      year={2021},\n",
        "      eprint={2108.00080},\n",
        "      archivePrefix={arXiv},\n",
        "      primaryClass={cs.CV}\n",
        "}\n",
        "\n",
        "@misc{huang2024detecting,\n",
        "      title={Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning},\n",
        "      author={Zhe Huang and Benjamin S. Wessler and Michael C. Hughes},\n",
        "      year={2024},\n",
        "      eprint={2306.00003},\n",
        "      archivePrefix={arXiv},\n",
        "      primaryClass={eess.IV}\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "id": "SHMI2chl9omn"
      }
    }
  ]
}